{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# toy example of using two stream metric learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:VERSION 0.9.84\n"
     ]
    }
   ],
   "source": [
    "# The testing module requires faiss\n",
    "# So if you don't have that, then this import will break \n",
    "from pytorch_metric_learning import losses, miners, samplers, trainers, testers\n",
    "import pytorch_metric_learning.utils.logging_presets as logging_presets\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from PIL import Image\n",
    "import pytorch_metric_learning\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.info(\"VERSION %s\"%pytorch_metric_learning.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR100TwoStreamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, anchor_transform, posneg_transform):\n",
    "        lengths = [int(len(dataset)*0.8), int(len(dataset)*0.2)]\n",
    "        self.anchors, self.posnegs = torch.utils.data.random_split(dataset, lengths)\n",
    "        \n",
    "        self.anchor_transform = anchor_transform\n",
    "        self.posneg_transform = posneg_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anchors)\n",
    "        \n",
    "    def __getitem__(self, index):            \n",
    "        anchor, target = self.anchors[index]\n",
    "        if self.anchor_transform is not None:\n",
    "            anchor = self.anchor_transform(anchor)\n",
    "        \n",
    "        # now pair this up with an image from the same class in the second stream\n",
    "        A = np.where( np.array(self.posnegs.dataset.targets)==target )[0]\n",
    "        posneg_idx = np.random.choice(A[np.in1d(A, self.posnegs.indices)])\n",
    "        posneg, target = self.posnegs[np.where(self.posnegs.indices==posneg_idx)[0][0]]\n",
    "        \n",
    "        if self.posneg_transform is not None:\n",
    "            posneg = self.posneg_transform(posneg)\n",
    "        return anchor, posneg, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a basic multilayer perceptron\n",
    "# This code is from https://github.com/KevinMusgrave/powerful_benchmarker\n",
    "class MLP(nn.Module):\n",
    "    # layer_sizes[0] is the dimension of the input\n",
    "    # layer_sizes[-1] is the dimension of the output\n",
    "    def __init__(self, layer_sizes, final_relu=False):\n",
    "        super().__init__()\n",
    "        layer_list = []\n",
    "        layer_sizes = [int(x) for x in layer_sizes]\n",
    "        num_layers = len(layer_sizes) - 1\n",
    "        final_relu_layer = num_layers if final_relu else num_layers - 1\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            input_size = layer_sizes[i]\n",
    "            curr_size = layer_sizes[i + 1]\n",
    "            if i < final_relu_layer:\n",
    "                layer_list.append(nn.ReLU(inplace=True))\n",
    "            layer_list.append(nn.Linear(input_size, curr_size))\n",
    "        self.net = nn.Sequential(*layer_list)\n",
    "        self.last_linear = self.net[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# This is for replacing the last layer of a pretrained network.\n",
    "# This code is from https://github.com/KevinMusgrave/powerful_benchmarker\n",
    "class Identity(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "# This code is from https://github.com/KevinMusgrave/powerful_benchmarker\n",
    "class ListOfModels(nn.Module):\n",
    "    def __init__(self, list_of_models, input_sizes=None, operation_before_concat=None):\n",
    "        super().__init__()\n",
    "        self.list_of_models = nn.ModuleList(list_of_models)\n",
    "        self.input_sizes = input_sizes\n",
    "        self.operation_before_concat = (lambda x: x) if not operation_before_concat else operation_before_concat\n",
    "        for k in [\"mean\", \"std\", \"input_space\", \"input_range\"]:\n",
    "            setattr(self, k, getattr(list_of_models[0], k, None))\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        if self.input_sizes is None:\n",
    "            for m in self.list_of_models:\n",
    "                curr_output = self.operation_before_concat(m(x))\n",
    "                outputs.append(curr_output)\n",
    "        else:\n",
    "            s = 0\n",
    "            for i, y in enumerate(self.input_sizes):\n",
    "                curr_input = x[:, s : s + y]\n",
    "                curr_output = self.operation_before_concat(self.list_of_models[i](curr_input))\n",
    "                outputs.append(curr_output)\n",
    "                s += y\n",
    "        return torch.cat(outputs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set trunk model and replace the softmax layer with an identity function\n",
    "trunk = models.resnet18(pretrained=True)\n",
    "trunk_output_size = trunk.fc.in_features\n",
    "trunk.fc = Identity()\n",
    "trunk = torch.nn.DataParallel(trunk.to(device))\n",
    "\n",
    "# Set embedder model. This takes in the output of the trunk and outputs 64 dimensional embeddings\n",
    "embedder = torch.nn.DataParallel(MLP([trunk_output_size, 128]).to(device))\n",
    "\n",
    "# Set optimizers\n",
    "trunk_optimizer = torch.optim.Adam(trunk.parameters(), lr=0.00004, weight_decay=0.00005)\n",
    "embedder_optimizer = torch.optim.Adam(embedder.parameters(), lr=0.00004, weight_decay=0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "posneg_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    ")\n",
    "\n",
    "anchor_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    ")\n",
    "\n",
    "# Set the datasets\n",
    "original_train = datasets.CIFAR100(root=\"../CIFAR100_Dataset\", train=True, transform=None, download=True)\n",
    "original_val = datasets.CIFAR100(root=\"../CIFAR100_Dataset\", train=False, transform=None, download=True)\n",
    "\n",
    "# splits CIFAR100 into two streams\n",
    "# 20% of the images will be used as a stream for positives and negatives\n",
    "# the remaining images are used as anchor images\n",
    "\n",
    "train_dataset = CIFAR100TwoStreamDataset(original_train, anchor_transform=anchor_transform, posneg_transform=posneg_transform)\n",
    "val_dataset = CIFAR100TwoStreamDataset(original_val, anchor_transform=anchor_transform, posneg_transform=posneg_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing dataloader\n",
      "INFO:root:Initializing dataloader iterator\n",
      "INFO:root:Done creating dataloader iterator\n",
      "INFO:root:TRAINING EPOCH 1\n",
      "total_loss=0.17232: 100%|██████████| 50/50 [00:30<00:00,  1.66it/s]\n",
      "INFO:root:Evaluating epoch 1\n",
      "INFO:root:Getting embeddings for the val split\n",
      "100%|██████████| 250/250 [00:11<00:00, 22.65it/s]\n",
      "INFO:root:Computing accuracy for the val split\n",
      "INFO:root:running k-nn with k=88\n",
      "INFO:root:embedding dimensionality is 128\n",
      "INFO:root:running k-means clustering with k=100\n",
      "INFO:root:embedding dimensionality is 128\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:732: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "INFO:root:New best accuracy!\n",
      "INFO:root:TRAINING EPOCH 2\n",
      "total_loss=0.16593: 100%|██████████| 50/50 [00:30<00:00,  1.68it/s]\n",
      "INFO:root:Evaluating epoch 2\n",
      "INFO:root:Getting embeddings for the val split\n",
      "100%|██████████| 250/250 [00:11<00:00, 21.31it/s]\n",
      "INFO:root:Computing accuracy for the val split\n",
      "INFO:root:running k-nn with k=88\n",
      "INFO:root:embedding dimensionality is 128\n",
      "INFO:root:running k-means clustering with k=100\n",
      "INFO:root:embedding dimensionality is 128\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:732: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "INFO:root:New best accuracy!\n",
      "INFO:root:TRAINING EPOCH 3\n",
      "total_loss=0.15927: 100%|██████████| 50/50 [00:30<00:00,  1.66it/s]\n",
      "INFO:root:Evaluating epoch 3\n",
      "INFO:root:Getting embeddings for the val split\n",
      "100%|██████████| 250/250 [00:13<00:00, 21.86it/s]\n",
      "INFO:root:Computing accuracy for the val split\n",
      "INFO:root:running k-nn with k=88\n",
      "INFO:root:embedding dimensionality is 128\n",
      "INFO:root:running k-means clustering with k=100\n",
      "INFO:root:embedding dimensionality is 128\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:732: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "INFO:root:New best accuracy!\n",
      "INFO:root:TRAINING EPOCH 4\n",
      "total_loss=0.16006: 100%|██████████| 50/50 [00:33<00:00,  1.36it/s]\n",
      "INFO:root:Evaluating epoch 4\n",
      "INFO:root:Getting embeddings for the val split\n",
      "100%|██████████| 250/250 [00:22<00:00, 11.14it/s]\n",
      "INFO:root:Computing accuracy for the val split\n",
      "INFO:root:running k-nn with k=88\n",
      "INFO:root:embedding dimensionality is 128\n",
      "INFO:root:running k-means clustering with k=100\n",
      "INFO:root:embedding dimensionality is 128\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:732: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "INFO:root:New best accuracy!\n",
      "INFO:root:TRAINING EPOCH 5\n",
      "total_loss=0.14362: 100%|██████████| 50/50 [00:34<00:00,  1.49it/s]\n",
      "INFO:root:Evaluating epoch 5\n",
      "INFO:root:Getting embeddings for the val split\n",
      "100%|██████████| 250/250 [00:13<00:00, 18.78it/s]\n",
      "INFO:root:Computing accuracy for the val split\n",
      "INFO:root:running k-nn with k=88\n",
      "INFO:root:embedding dimensionality is 128\n",
      "INFO:root:running k-means clustering with k=100\n",
      "INFO:root:embedding dimensionality is 128\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:732: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "INFO:root:New best accuracy!\n",
      "INFO:root:TRAINING EPOCH 6\n",
      "total_loss=0.15490: 100%|██████████| 50/50 [00:45<00:00,  1.41it/s]\n",
      "INFO:root:Evaluating epoch 6\n",
      "INFO:root:Getting embeddings for the val split\n",
      "100%|██████████| 250/250 [00:17<00:00, 14.63it/s]\n",
      "INFO:root:Computing accuracy for the val split\n",
      "INFO:root:running k-nn with k=88\n",
      "INFO:root:embedding dimensionality is 128\n",
      "INFO:root:running k-means clustering with k=100\n",
      "INFO:root:embedding dimensionality is 128\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:732: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "INFO:root:New best accuracy!\n",
      "INFO:root:TRAINING EPOCH 7\n",
      "total_loss=0.14244: 100%|██████████| 50/50 [00:30<00:00,  1.68it/s]\n",
      "INFO:root:Evaluating epoch 7\n",
      "INFO:root:Getting embeddings for the val split\n",
      "100%|██████████| 250/250 [00:14<00:00, 18.62it/s]\n",
      "INFO:root:Computing accuracy for the val split\n",
      "INFO:root:running k-nn with k=88\n",
      "INFO:root:embedding dimensionality is 128\n",
      "INFO:root:running k-means clustering with k=100\n",
      "INFO:root:embedding dimensionality is 128\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:732: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "INFO:root:New best accuracy!\n",
      "INFO:root:TRAINING EPOCH 8\n",
      "total_loss=0.14372: 100%|██████████| 50/50 [00:31<00:00,  1.44it/s]\n",
      "INFO:root:Evaluating epoch 8\n",
      "INFO:root:Getting embeddings for the val split\n",
      "100%|██████████| 250/250 [00:14<00:00, 21.71it/s]\n",
      "INFO:root:Computing accuracy for the val split\n",
      "INFO:root:running k-nn with k=88\n",
      "INFO:root:embedding dimensionality is 128\n",
      "INFO:root:running k-means clustering with k=100\n",
      "INFO:root:embedding dimensionality is 128\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:732: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:New best accuracy!\n",
      "INFO:root:TRAINING EPOCH 9\n",
      "total_loss=0.14181: 100%|██████████| 50/50 [00:31<00:00,  1.48it/s]\n",
      "INFO:root:Evaluating epoch 9\n",
      "INFO:root:Getting embeddings for the val split\n",
      "100%|██████████| 250/250 [00:13<00:00, 18.32it/s]\n",
      "INFO:root:Computing accuracy for the val split\n",
      "INFO:root:running k-nn with k=88\n",
      "INFO:root:embedding dimensionality is 128\n",
      "INFO:root:running k-means clustering with k=100\n",
      "INFO:root:embedding dimensionality is 128\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:732: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "INFO:root:New best accuracy!\n",
      "INFO:root:TRAINING EPOCH 10\n",
      "total_loss=0.13159: 100%|██████████| 50/50 [00:31<00:00,  1.36it/s]\n",
      "INFO:root:Evaluating epoch 10\n",
      "INFO:root:Getting embeddings for the val split\n",
      "100%|██████████| 250/250 [00:17<00:00, 14.58it/s]\n",
      "INFO:root:Computing accuracy for the val split\n",
      "INFO:root:running k-nn with k=88\n",
      "INFO:root:embedding dimensionality is 128\n",
      "INFO:root:running k-means clustering with k=100\n",
      "INFO:root:embedding dimensionality is 128\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:732: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/home/marijn/anaconda3/envs/pytorch/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "INFO:root:New best accuracy!\n"
     ]
    }
   ],
   "source": [
    "# Set the loss function\n",
    "loss = losses.TripletMarginLoss(margin=0.2)\n",
    "\n",
    "# Set the mining function\n",
    "miner = miners.TripletMarginMiner(margin=0.2)\n",
    "\n",
    "# Set the dataloader sampler\n",
    "sampler = samplers.MPerClassSampler(original_train.classes, m=1)\n",
    "\n",
    "# Set other training parameters\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "iterations_per_epoch = 50\n",
    "\n",
    "# Package the above stuff into dictionaries.\n",
    "models = {\"trunk\": trunk, \"embedder\": embedder}\n",
    "optimizers = {\"trunk_optimizer\": trunk_optimizer, \"embedder_optimizer\": embedder_optimizer}\n",
    "loss_funcs = {\"metric_loss\": loss}\n",
    "mining_funcs = {\"tuple_miner\": miner}\n",
    "\n",
    "record_keeper, _, _ = logging_presets.get_record_keeper(\"example_logs\", \"example_tensorboard\")\n",
    "hooks = logging_presets.get_hook_container(record_keeper)\n",
    "dataset_dict = {\"val\": val_dataset}\n",
    "model_folder = \"example_saved_models\"\n",
    "\n",
    "# Create the tester\n",
    "tester = testers.GlobalTwoStreamEmbeddingSpaceTester(end_of_testing_hook=hooks.end_of_testing_hook, dataloader_num_workers=2)\n",
    "end_of_epoch_hook = hooks.end_of_epoch_hook(tester, dataset_dict, model_folder)\n",
    "\n",
    "trainer = trainers.TwoStreamMetricLoss(models=models,\n",
    "                                optimizers=optimizers,\n",
    "                                batch_size=batch_size,\n",
    "                                loss_funcs=loss_funcs,\n",
    "                                mining_funcs=mining_funcs,\n",
    "                                iterations_per_epoch=iterations_per_epoch,\n",
    "                                dataset=train_dataset,\n",
    "                                sampler=sampler,\n",
    "                                dataloader_num_workers=2,\n",
    "                                end_of_iteration_hook=hooks.end_of_iteration_hook,\n",
    "                                end_of_epoch_hook=end_of_epoch_hook\n",
    "                                )\n",
    "\n",
    "trainer.train(num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python36764bitpytorchcondaa849f917647d4bbc9fa67d6d2d325edc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
