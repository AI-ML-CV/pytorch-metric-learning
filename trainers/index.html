



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.dev0, mkdocs-material-4.6.0">
    
    
      
        <title>Trainers - PyTorch Metric Learning</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.1b62728e.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.a8b3c06d.css">
      
      
        
        
        <meta name="theme-color" content="#ef5350">
      
    
    
      <script src="../assets/javascripts/modernizr.268332fc.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="red" data-md-color-accent="red">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#trainers" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href=".." title="PyTorch Metric Learning" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              PyTorch Metric Learning
            </span>
            <span class="md-header-nav__topic">
              
                Trainers
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/KevinMusgrave/pytorch-metric-learning/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    KevinMusgrave/pytorch-metric-learning
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href=".." title="PyTorch Metric Learning" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    PyTorch Metric Learning
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/KevinMusgrave/pytorch-metric-learning/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    KevinMusgrave/pytorch-metric-learning
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../losses/" title="Losses" class="md-nav__link">
      Losses
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../miners/" title="Miners" class="md-nav__link">
      Miners
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../reducers/" title="Reducers" class="md-nav__link">
      Reducers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../regularizers/" title="Regularizers" class="md-nav__link">
      Regularizers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../samplers/" title="Samplers" class="md-nav__link">
      Samplers
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Trainers
      </label>
    
    <a href="./" title="Trainers" class="md-nav__link md-nav__link--active">
      Trainers
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basetrainer" class="md-nav__link">
    BaseTrainer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metriclossonly" class="md-nav__link">
    MetricLossOnly
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#trainwithclassifier" class="md-nav__link">
    TrainWithClassifier
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cascadedembeddings" class="md-nav__link">
    CascadedEmbeddings
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deepadversarialmetriclearning" class="md-nav__link">
    DeepAdversarialMetricLearning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unsupervisedembeddingsusingaugmentations" class="md-nav__link">
    UnsupervisedEmbeddingsUsingAugmentations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#twostreammetricloss" class="md-nav__link">
    TwoStreamMetricLoss
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../testers/" title="Testers" class="md-nav__link">
      Testers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-9" type="checkbox" id="nav-9">
    
    <label class="md-nav__link" for="nav-9">
      Utils
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-9">
        Utils
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../accuracy_calculation/" title="Accuracy Calculation" class="md-nav__link">
      Accuracy Calculation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../inference_models/" title="Inference Models" class="md-nav__link">
      Inference Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../logging_presets/" title="Logging Presets" class="md-nav__link">
      Logging Presets
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-10" type="checkbox" id="nav-10">
    
    <label class="md-nav__link" for="nav-10">
      How to extend this library
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-10">
        How to extend this library
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../losses_dev/" title="Custom losses" class="md-nav__link">
      Custom losses
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basetrainer" class="md-nav__link">
    BaseTrainer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#metriclossonly" class="md-nav__link">
    MetricLossOnly
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#trainwithclassifier" class="md-nav__link">
    TrainWithClassifier
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#cascadedembeddings" class="md-nav__link">
    CascadedEmbeddings
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deepadversarialmetriclearning" class="md-nav__link">
    DeepAdversarialMetricLearning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unsupervisedembeddingsusingaugmentations" class="md-nav__link">
    UnsupervisedEmbeddingsUsingAugmentations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#twostreammetricloss" class="md-nav__link">
    TwoStreamMetricLoss
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/edit/master/docs/trainers.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="trainers">Trainers<a class="headerlink" href="#trainers" title="Permanent link">&para;</a></h1>
<p>Trainers exist in this library because some metric learning algorithms are more than just loss or mining functions. Some algorithms require additional networks, data augmentations, learning rate schedules etc. The goal of the trainers module is to provide access to these type of metric learning algorithms. </p>
<p>In general, trainers are used as follows:</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_metric_learning</span> <span class="kn">import</span> <span class="n">trainers</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">trainers</span><span class="o">.</span><span class="n">SomeTrainingFunction</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">t</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>


<h2 id="basetrainer">BaseTrainer<a class="headerlink" href="#basetrainer" title="Permanent link">&para;</a></h2>
<p>All trainers extend this class and therefore inherit its <code>__init__</code> arguments.</p>
<div class="codehilite"><pre><span></span><span class="n">trainers</span><span class="o">.</span><span class="n">BaseTrainer</span><span class="p">(</span><span class="n">models</span><span class="p">,</span>
                    <span class="n">optimizers</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="p">,</span>
                    <span class="n">loss_funcs</span><span class="p">,</span>
                    <span class="n">mining_funcs</span><span class="p">,</span>
                    <span class="n">dataset</span><span class="p">,</span>
                    <span class="n">iterations_per_epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">data_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">loss_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">collate_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">lr_schedulers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">gradient_clippers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">freeze_these</span><span class="o">=</span><span class="p">(),</span>
                    <span class="n">freeze_trunk_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">label_hierarchy_level</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">dataloader_num_workers</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">data_and_label_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">dataset_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">set_min_label_to_zero</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">end_of_iteration_hook</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">end_of_epoch_hook</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>models</strong>: A dictionary of the form: <ul>
<li>{"trunk": trunk_model, "embedder": embedder_model}</li>
<li>The "embedder" key is optional.</li>
</ul>
</li>
<li><strong>optimizers</strong>: A dictionary mapping strings to optimizers. The base class does not require any specific keys. For example, you could provide an empty dictionary, in which case no optimization will happen. Or you could provide just an optimizer for your trunk_model. But most likely, you'll want to pass in: <ul>
<li>{"trunk_optimizer": trunk_optimizer, "embedder_optimizer": embedder_optimizer}.</li>
</ul>
</li>
<li><strong>batch_size</strong>: The number of elements that are retrieved at each iteration.</li>
<li><strong>loss_funcs</strong>: A dictionary mapping strings to loss functions. The required keys depend on the training method, but all methods are likely to require at least: <ul>
<li>{"metric_loss": loss_func}.</li>
</ul>
</li>
<li><strong>mining_funcs</strong>: A dictionary mapping strings to mining functions. Pass in an empty dictionary, or one or more of the following keys: <ul>
<li>{"subset_batch_miner": mining_func1, "tuple_miner": mining_func2}</li>
</ul>
</li>
<li><strong>dataset</strong>: The dataset you want to train on. Note that training methods do not perform validation, so do not pass in your validation or test set.</li>
<li><strong>data_device</strong>: The device that you want to put batches of data on. If not specified, the trainer will put the data on any available GPUs.</li>
<li><strong>iterations_per_epoch</strong>: Optional. <ul>
<li>If you don't specify <code>iterations_per_epoch</code>:<ul>
<li>1 epoch = 1 pass through the dataloader iterator. If <code>sampler=None</code>, then 1 pass through the iterator is 1 pass through the dataset. </li>
<li>If you use a sampler, then 1 pass through the iterator is 1 pass through the iterable returned by the sampler.</li>
</ul>
</li>
<li>For samplers like <code>MPerClassSampler</code> or some offline mining method, the iterable returned might be very long or very short etc, and might not be related to the length of the dataset. The length of the epoch might vary each time the sampler creates a new iterable. In these cases, it can be useful to specify <code>iterations_per_epoch</code> so that each "epoch" is just a fixed number of iterations. The definition of epoch matters because there's various things like LR schedulers and hooks that depend on an epoch ending.</li>
</ul>
</li>
<li><strong>loss_weights</strong>: A dictionary mapping loss names to numbers. Each loss will be multiplied by the corresponding value in the dictionary. If not specified, then no loss weighting will occur.
If not specified, then the original labels are used.</li>
<li><strong>sampler</strong>: The sampler used by the dataloader. If not specified, then random sampling will be used.</li>
<li><strong>collate_fn</strong>: The collate function used by the dataloader.</li>
<li><strong>lr_scheduers</strong>: A dictionary of PyTorch learning rate schedulers. Your keys should be strings of the form <code>&lt;model&gt;_&lt;step_type&gt;</code>, where <code>&lt;model&gt;</code> is a key that comes from either the <code>models</code> or <code>loss_funcs</code> dictionary, and <code>&lt;step_type&gt;</code> is one of the following:<ul>
<li>"scheduler_by_iteration" (will be stepped at every iteration)</li>
<li>"scheduler_by_epoch" (will be stepped at the end of each epoch)</li>
<li>"scheduler_by_plateau" (will step if accuracy plateaus. This requires you to write your own end-of-epoch hook, compute validation accuracy, and call <code>trainer.step_lr_plateau_schedulers(validation_accuracy)</code>. Or you can use <a href="../logging_presets/">HookContainer</a>.)</li>
<li>Here are some example valid <code>lr_scheduler</code> keys: <ul>
<li><code>trunk_scheduler_by_iteration</code></li>
<li><code>metric_loss_scheduler_by_epoch</code></li>
<li><code>embedder_scheduler_by_plateau</code></li>
</ul>
</li>
</ul>
</li>
<li><strong>gradient_clippers</strong>: A dictionary of gradient clipping functions. Each function will be called before the optimizers.</li>
<li><strong>freeze_these</strong>: Optional. A list or tuple of the names of models or loss functions that should have their parameters frozen during training. These models will have <code>requires_grad</code> set to False, and their optimizers will not be stepped. </li>
<li><strong>freeze_trunk_batchnorm</strong>: If True, then the BatchNorm parameters of the trunk model will be frozen during training.</li>
<li><strong>label_hierarchy_level</strong>: If each sample in your dataset has multiple labels, then this integer argument can be used to select which "level" to use. This assumes that your labels are "2-dimensional" with shape (num_samples, num_hierarchy_levels). Leave this at the default value, 0, if your data does not have multiple labels per sample.</li>
<li><strong>dataloader_num_workers</strong>: The number of processes your dataloader will use to load data.</li>
<li><strong>data_and_label_getter</strong>: A function that takes the output of your dataset's <code>__getitem__</code> function, and returns a tuple of (data, labels). If None, then it is assumed that <code>__getitem__</code> returns (data, labels). </li>
<li><strong>dataset_labels</strong>: The labels for your dataset. Can be 1-dimensional (1 label per datapoint) or 2-dimensional, where each row represents a datapoint, and the columns are the multiple labels that the datapoint has. Labels can be integers or strings. <strong>This option needs to be specified only if <code>set_min_label_to_zero</code> is True.</strong></li>
<li><strong>set_min_label_to_zero</strong>: If True, labels will be mapped such that they represent their rank in the label set. For example, if your dataset has labels 5, 10, 12, 13, then at each iteration, these would become 0, 1, 2, 3. You should also set this to True if you want to use string labels. In that case, 'dog', 'cat', 'monkey' would get mapped to 1, 0, 2. If True, you must pass in <code>dataset_labels</code> (see above). The default is False.</li>
<li><strong>end_of_iteration_hook</strong>: This is an optional function that has one input argument (the trainer object), and performs some action (e.g. logging data) at the end of every iteration. Here are some things you might want to log:<ul>
<li><code>trainer.losses</code>: this dictionary contains all loss values at the current iteration. </li>
<li><code>trainer.loss_funcs</code> and <code>trainer.mining_funcs</code>: these dictionaries contain the loss and mining functions. <ul>
<li>All loss and mining functions in pytorch-metric-learning have an attribute called <code>record_these</code>. This attribute is a list of strings, which are the names of other attributes that are worth recording for the purpose of analysis. For example, the <code>record_these</code> list for TripletMarginLoss is <code>["avg_embedding_norm, "num_non_zero_triplets"]</code>, so at each iteration you could log the value of <code>trainer.loss_funcs["metric_loss"].avg_embedding_norm</code> and <code>trainer.loss_funcs["metric_loss"].num_non_zero_triplets</code>. To accomplish this programmatically, you can loop through <code>record_these</code> and use the python function <code>getattr</code> to retrieve the attribute value.</li>
</ul>
</li>
<li>If you want ready-to-use hooks, take a look at the <a href="../logging_presets/">logging_presets module</a>.</li>
</ul>
</li>
<li><strong>end_of_epoch_hook</strong>: This is an optional function that operates like <code>end_of_iteration_hook</code>, except this occurs at the end of every epoch, so this might be a suitable place to run validation and save models. <ul>
<li>To end training early, your hook should return the boolean value False. Note, it must specifically <code>return False</code>, not <code>None</code>, <code>0</code>, <code>[]</code> etc.</li>
<li>For this hook, you might want to access the following dictionaries: <code>trainer.models</code>, <code>trainer.optimizers</code>, <code>trainer.lr_schedulers</code>, <code>trainer.loss_funcs</code>, and <code>trainer.mining_funcs</code>.</li>
<li>If you want ready-to-use hooks, take a look at the <a href="../logging_presets/">logging_presets module</a>.</li>
</ul>
</li>
</ul>
<h2 id="metriclossonly">MetricLossOnly<a class="headerlink" href="#metriclossonly" title="Permanent link">&para;</a></h2>
<p>This trainer just computes a metric loss from the output of your embedder network. See <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/MetricLossOnly.ipynb">the example notebook</a>.</p>
<div class="codehilite"><pre><span></span><span class="n">trainers</span><span class="o">.</span><span class="n">MetricLossOnly</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Requirements</strong>:</p>
<ul>
<li><strong>models</strong>: Must have the following form:<ul>
<li>{"trunk": trunk_model}</li>
<li>Optionally include "embedder": embedder_model</li>
</ul>
</li>
<li><strong>loss_funcs</strong>: Must have the following form:<ul>
<li>{"metric_loss": loss_func}</li>
</ul>
</li>
</ul>
<h2 id="trainwithclassifier">TrainWithClassifier<a class="headerlink" href="#trainwithclassifier" title="Permanent link">&para;</a></h2>
<p>This trainer is for the case where your architecture is trunk -&gt; embedder -&gt; classifier. It applies a metric loss to the output of the embedder network, and a classification loss to the output of the classifier network. See <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/TrainWithClassifier.ipynb">the example notebook</a>.</p>
<div class="codehilite"><pre><span></span><span class="n">trainers</span><span class="o">.</span><span class="n">TrainWithClassifier</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Requirements</strong>:</p>
<ul>
<li><strong>models</strong>: Must have the following form: <ul>
<li>{"trunk": trunk_model, "classifier": classifier_model}</li>
<li>Optionally include "embedder": embedder_model</li>
</ul>
</li>
<li><strong>loss_funcs</strong>: Must have the following form:<ul>
<li>{"metric_loss": loss_func1, "classifier_loss": loss_func2}</li>
</ul>
</li>
</ul>
<h2 id="cascadedembeddings">CascadedEmbeddings<a class="headerlink" href="#cascadedembeddings" title="Permanent link">&para;</a></h2>
<p>This trainer is a generalization of <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Yuan_Hard-Aware_Deeply_Cascaded_ICCV_2017_paper.pdf">Hard-Aware Deeply Cascaded Embedding</a>. It splits the output of your embedder network, computing a separate loss for each section. In other words, the output of your embedder should be the concatenation of your cascaded models. See <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/CascadedEmbeddings.ipynb">the example notebook</a>.</p>
<div class="codehilite"><pre><span></span><span class="n">trainers</span><span class="o">.</span><span class="n">CascadedEmbeddings</span><span class="p">(</span><span class="n">embedding_sizes</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li>embedding_sizes: A list of integers, which represent the size of the output of each cascaded model.</li>
</ul>
<p><strong>Requirements</strong>:</p>
<ul>
<li>
<p><strong>models</strong>: Must have the following form:</p>
<ul>
<li>{"trunk": trunk_model}<ul>
<li>Optionally include "embedder": embedder_model</li>
<li>Optionally include key:values of the form "classifier_%d": classifier_model_%d. The integer appended to "classifier_" represents the cascaded model that the classifier will be appended to. For example, if the dictionary has classifier_0 and classifier_2, then the 0th and 2nd cascaded models will have classifier_model_0 and classifier_model_2 appended to them.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>loss_funcs</strong>: Must have the following form:</p>
<ul>
<li>{"metric_loss_%d": metric_loss_func_%d}<ul>
<li>Optionally include key:values of the form "classifier_loss_%d": classifier_loss_func_%d. The appended integer represents which cascaded model the loss applies to.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>mining_funcs</strong>: Must have the following form:</p>
<ul>
<li>{"tuple_miner_%d": mining_func_%d}<ul>
<li>Optionally include "subset_batch_miner": subset_batch_miner</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="deepadversarialmetriclearning">DeepAdversarialMetricLearning<a class="headerlink" href="#deepadversarialmetriclearning" title="Permanent link">&para;</a></h2>
<p>This is an implementation of <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_Deep_Adversarial_Metric_CVPR_2018_paper.pdf">Deep Adversarial Metric Learning</a>. See <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/DeepAdversarialMetricLearning.ipynb">the example notebook</a>.</p>
<div class="codehilite"><pre><span></span><span class="n">trainers</span><span class="o">.</span><span class="n">DeepAdversarialMetricLearning</span><span class="p">(</span><span class="n">metric_alone_epochs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                    <span class="n">g_alone_epochs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                    <span class="n">g_triplets_per_anchor</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                                    <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>metric_alone_epochs</strong>: At the beginning of training, this many epochs will consist of only the metric_loss.</li>
<li><strong>g_alone_epochs</strong>: After metric_alone_epochs, this many epochs will consist of only the adversarial generator loss.</li>
<li><strong>g_triplets_per_anchor</strong>: The number of real triplets per sample that should be passed into the generator. For each real triplet, the generator will output a synthetic triplet.</li>
</ul>
<p><strong>Requirements</strong>:</p>
<ul>
<li>
<p><strong>models</strong>: Must have the following form:</p>
<ul>
<li>{"trunk": trunk_model, "generator": generator_model}<ul>
<li>Optionally include "embedder": embedder_model</li>
<li>Optionally include "classifier": classifier_model</li>
<li>The input size to the generator must be 3*(size of trunk_model output). The output size must be (size of trunk_model output).</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>loss_funcs</strong>: Must have the following form:</p>
<ul>
<li>{"metric_loss": metric_loss, "g_adv_loss": g_adv_loss, "synth_loss": synth_loss}<ul>
<li>Optionally include "classifier_loss": classifier_loss</li>
<li>metric_loss applies to the embeddings of real data.</li>
<li>g_adv_loss is the adversarial generator loss. <strong>Currently, only TripletMarginLoss and AngularLoss are supported</strong></li>
<li>synth_loss applies to the embeddings of the synthetic generator triplets.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>loss_weights</strong>: Must be one of the following:</p>
<ul>
<li>None</li>
<li>{"metric_loss": weight1, "g_adv_loss": weight2, "synth_loss": weight3, "g_reg_loss": weight4, "g_hard_lss": weight5}<ul>
<li>Optionally include "classifier_loss": classifier_loss</li>
<li>"g_reg_loss" and "g_hard_loss" refer to the regularization losses described in the paper.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="unsupervisedembeddingsusingaugmentations">UnsupervisedEmbeddingsUsingAugmentations<a class="headerlink" href="#unsupervisedembeddingsusingaugmentations" title="Permanent link">&para;</a></h2>
<p>This is an implementation of a general approach that has been used in recent unsupervised learning papers, e.g. <a href="https://arxiv.org/pdf/1904.03436.pdf">Unsupervised Embedding Learning via Invariant and Spreading
Instance Feature
</a> and <a href="https://arxiv.org/abs/1911.07072">Unsupervised Deep Metric Learning via Auxiliary Rotation Loss</a>. The idea is that augmented versions of a datapoint should be close to each other in the embedding space.</p>
<div class="codehilite"><pre><span></span><span class="n">trainers</span><span class="o">.</span><span class="n">UnsupervisedEmbeddingsUsingAugmentations</span><span class="p">(</span><span class="n">transforms</span><span class="p">,</span> <span class="n">data_and_label_setter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Parameters</strong>:</p>
<ul>
<li><strong>transforms</strong>: A list of transforms. For every sample in a batch, each transform will be applied to the sample. If there are N transforms and the batch size is B, then there will be a total of B*N augmented samples. </li>
<li><strong>data_and_label_setter</strong>: A function that takes in a tuple of (augmented_data, pseudo_labels) and outputs whatever is expected by self.data_and_label_getter.</li>
</ul>
<h2 id="twostreammetricloss">TwoStreamMetricLoss<a class="headerlink" href="#twostreammetricloss" title="Permanent link">&para;</a></h2>
<p>This trainer is the same as <a href="./#metriclossonly">MetricLossOnly</a> but operates on separate streams of anchors and positives/negatives.
The supplied <strong>dataset</strong> must return <code>(anchor, positive, label)</code>.
Given a batch of <code>(anchor, positive, label)</code>, triplets are formed using <code>anchor</code> as the anchor, and <code>positive</code> as either the positive or negative. See <a href="https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/TwoStreamMetricLoss.ipynb">the example notebook</a>.</p>
<div class="codehilite"><pre><span></span><span class="n">trainers</span><span class="o">.</span><span class="n">TwoStreamMetricLoss</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>


<p><strong>Requirements</strong>:</p>
<ul>
<li><strong>models</strong>: Must have the following form:<ul>
<li>{"trunk": trunk_model}</li>
<li>Optionally include "embedder": embedder_model</li>
</ul>
</li>
<li><strong>loss_funcs</strong>: Must have the following form:<ul>
<li>{"metric_loss": loss_func}</li>
</ul>
</li>
<li><strong>mining_funcs</strong>: Only tuple miners are supported</li>
</ul>
                
                  
                
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../samplers/" title="Samplers" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Samplers
              </span>
            </div>
          </a>
        
        
          <a href="../testers/" title="Testers" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Testers
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.808e90bb.js"></script>
      
      <script>app.initialize({version:"1.1.dev0",url:{base:".."}})</script>
      
    
  </body>
</html>