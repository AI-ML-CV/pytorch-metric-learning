{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTorch Metric Learning \u00b6 Google Colab Examples \u00b6 See the examples folder for notebooks that show entire train/test workflows with logging and model saving. Installation \u00b6 Pip \u00b6 pip install pytorch-metric-learning To get the latest dev version : pip install pytorch-metric-learning==0.9.86.dev1 To install on Windows : pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html pip install pytorch-metric-learning To install with evaluation and logging capabilities : pip install pytorch-metric-learning[with-hooks] To install with evaluation and logging capabilities (CPU) pip install pytorch-metric-learning[with-hooks-cpu] Conda \u00b6 conda install pytorch-metric-learning -c metric-learning We have recently noticed some sporadic issues with the conda installation, so we recommend installing with pip. You can use pip inside of conda: conda install pip pip install pytorch-metric-learning If you run into problems during installation, please post in this issue . Overview \u00b6 Let\u2019s try the vanilla triplet margin loss . In all examples, embeddings is assumed to be of size (N, embedding_size), and labels is of size (N). from pytorch_metric_learning import losses loss_func = losses . TripletMarginLoss ( margin = 0.1 ) loss = loss_func ( embeddings , labels ) Loss functions typically come with a variety of parameters. For example, with the TripletMarginLoss, you can control how many triplets per sample to use in each batch. You can also use all possible triplets within each batch: loss_func = losses . TripletMarginLoss ( triplets_per_anchor = \"all\" ) Sometimes it can help to add a mining function: from pytorch_metric_learning import miners , losses miner = miners . MultiSimilarityMiner ( epsilon = 0.1 ) loss_func = losses . TripletMarginLoss ( margin = 0.1 ) hard_pairs = miner ( embeddings , labels ) loss = loss_func ( embeddings , labels , hard_pairs ) In the above code, the miner finds positive and negative pairs that it thinks are particularly difficult. Note that even though the TripletMarginLoss operates on triplets, it\u2019s still possible to pass in pairs. This is because the library automatically converts pairs to triplets and triplets to pairs, when necessary. In general, all loss functions take in embeddings and labels, with an optional indices_tuple argument (i.e. the output of a miner): # From BaseMetricLossFunction def forward ( self , embeddings , labels , indices_tuple = None ) And (almost) all mining functions take in embeddings and labels: # From BaseMiner def forward ( self , embeddings , labels ) For more complex approaches, like deep adversarial metric learning, use one of the trainers . To check the accuracy of your model, use one of the testers . Which tester should you use? Almost definitely GlobalEmbeddingSpaceTester , because it does what most metric-learning papers do. Also check out the example Google Colab notebooks .","title":"Home"},{"location":"#pytorch-metric-learning","text":"","title":"PyTorch Metric Learning"},{"location":"#google-colab-examples","text":"See the examples folder for notebooks that show entire train/test workflows with logging and model saving.","title":"Google Colab Examples"},{"location":"#installation","text":"","title":"Installation"},{"location":"#pip","text":"pip install pytorch-metric-learning To get the latest dev version : pip install pytorch-metric-learning==0.9.86.dev1 To install on Windows : pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html pip install pytorch-metric-learning To install with evaluation and logging capabilities : pip install pytorch-metric-learning[with-hooks] To install with evaluation and logging capabilities (CPU) pip install pytorch-metric-learning[with-hooks-cpu]","title":"Pip"},{"location":"#conda","text":"conda install pytorch-metric-learning -c metric-learning We have recently noticed some sporadic issues with the conda installation, so we recommend installing with pip. You can use pip inside of conda: conda install pip pip install pytorch-metric-learning If you run into problems during installation, please post in this issue .","title":"Conda"},{"location":"#overview","text":"Let\u2019s try the vanilla triplet margin loss . In all examples, embeddings is assumed to be of size (N, embedding_size), and labels is of size (N). from pytorch_metric_learning import losses loss_func = losses . TripletMarginLoss ( margin = 0.1 ) loss = loss_func ( embeddings , labels ) Loss functions typically come with a variety of parameters. For example, with the TripletMarginLoss, you can control how many triplets per sample to use in each batch. You can also use all possible triplets within each batch: loss_func = losses . TripletMarginLoss ( triplets_per_anchor = \"all\" ) Sometimes it can help to add a mining function: from pytorch_metric_learning import miners , losses miner = miners . MultiSimilarityMiner ( epsilon = 0.1 ) loss_func = losses . TripletMarginLoss ( margin = 0.1 ) hard_pairs = miner ( embeddings , labels ) loss = loss_func ( embeddings , labels , hard_pairs ) In the above code, the miner finds positive and negative pairs that it thinks are particularly difficult. Note that even though the TripletMarginLoss operates on triplets, it\u2019s still possible to pass in pairs. This is because the library automatically converts pairs to triplets and triplets to pairs, when necessary. In general, all loss functions take in embeddings and labels, with an optional indices_tuple argument (i.e. the output of a miner): # From BaseMetricLossFunction def forward ( self , embeddings , labels , indices_tuple = None ) And (almost) all mining functions take in embeddings and labels: # From BaseMiner def forward ( self , embeddings , labels ) For more complex approaches, like deep adversarial metric learning, use one of the trainers . To check the accuracy of your model, use one of the testers . Which tester should you use? Almost definitely GlobalEmbeddingSpaceTester , because it does what most metric-learning papers do. Also check out the example Google Colab notebooks .","title":"Overview"},{"location":"losses/","text":"Losses \u00b6 All loss functions are used as follows: from pytorch_metric_learning import losses loss_func = losses . SomeLoss () loss = loss_func ( embeddings , labels ) Or if you are using a loss in conjunction with a miner: from pytorch_metric_learning import miners , losses miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) losses = loss_func ( embeddings , labels , miner_output ) AngularLoss \u00b6 Deep Metric Learning with Angular Loss losses . AngularLoss ( alpha , ** kwargs ) Parameters : alpha : The angle (as described in the paper), specified in degrees. ArcFaceLoss \u00b6 ArcFace: Additive Angular Margin Loss for Deep Face Recognition losses . ArcFaceLoss ( margin , num_classes , embedding_size , scale = 64 , ** kwargs ) Parameters : margin : The angular margin penalty in degrees. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) Other info : This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . ArcFaceLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () BaseMetricLossFunction \u00b6 All loss functions extend this class and therefore inherit its __init__ parameters. losses . BaseMetricLossFunction ( normalize_embeddings = True , num_class_per_param = None , learnable_param_names = None ) Parameters : normalize_embeddings : If True, embeddings will be normalized to have a Euclidean norm of 1 before the loss is computed. num_class_per_param : If learnable_param_names is set, then this represents the number of classes for each parameter. If your parameters don't have a separate value for each class, then you can leave this at None. learnable_param_names : A list of strings where each element is the name of attributes that should be converted to nn.Parameter. If None, then no parameters are converted. Required Implementations : def compute_loss ( self , embeddings , labels , indices_tuple = None ): raise NotImplementedError CircleLoss \u00b6 Circle Loss: A Unified Perspective of Pair Similarity Optimization losses . CircleLoss ( m = 0.4 , gamma = 80 , triplets_per_anchor = 'all' , ** kwargs ) Parameters : m : The relaxation factor that controls the radious of the decision boundary. gamma : The scale factor that determines the largest scale of each similarity score. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. ContrastiveLoss \u00b6 losses . ContrastiveLoss ( pos_margin = 0 , neg_margin = 1 , use_similarity = False , power = 1 , avg_non_zero_only = True , ** kwargs ): Parameters : pos_margin : The distance (or similarity) over (under) which positive pairs will contribute to the loss. neg_margin : The distance (or similarity) under (over) which negative pairs will contribute to the loss. use_similarity : If True, will use dot product between vectors instead of euclidean distance. power : Each pair's loss will be raised to this power. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss. Note that the default values for pos_margin and neg_margin are suitable if use_similarity = False . If you set use_similarity = True , then more appropriate values would be pos_margin = 1 and neg_margin = 0 . CosFaceLoss \u00b6 CosFace: Large Margin Cosine Loss for Deep Face Recognition losses . CosFaceLoss ( margin , num_classes , embedding_size , scale = 64 , ** kwargs ) Parameters : margin : The cosine margin penalty: cos(theta) - margin . The paper got optimal performance with margin values between 0.25 and 0.45. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) Other info : This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . CosFaceLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () CrossBatchMemory \u00b6 This wraps a loss function, and implements Cross-Batch Memory for Embedding Learning . It stores embeddings from previous iterations in a queue, and uses them to form more pairs/triplets with the current iteration's embeddings. losses . CrossBatchMemory ( loss , embedding_size , memory_size = 1024 , miner = None ) Parameters : loss : The loss function to be wrapped. For example, you could pass in ContrastiveLoss() . embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. memory_size : The size of the memory queue. miner : An optional tuple miner , which will be used to mine pairs/triplets from the memory queue. FastAPLoss \u00b6 Deep Metric Learning to Rank losses . FastAPLoss ( num_bins , ** kwargs ) Parameters : num_bins : The number of soft histogram bins for calculating average precision GenericPairLoss \u00b6 losses . GenericPairLoss ( use_similarity , mat_based_loss , squared_distances = False , ** kwargs ) Parameters : use_similarity : Set to True if the loss function uses pairwise similarity (dot product of each embedding pair). Otherwise, euclidean distance will be used. mat_based_loss : See required implementations. squared_distances : If True, then the euclidean distance will be squared. Required Implementations : # If mat_based_loss is True, then this takes in mat, pos_mask, and neg_mask # If False, this takes in pos_pairs and neg_pairs def _compute_loss ( self ): raise NotImplementedError GeneralizedLiftedStructureLoss \u00b6 losses . GeneralizedLiftedStructureLoss ( neg_margin , ** kwargs ) Parameters : neg_margin : The margin in the expression e^(margin - negative_distance) IntraPairVarianceLoss \u00b6 Deep Metric Learning with Tuplet Margin Loss losses . IntraPairVarianceLoss ( pos_eps = 0.01 , neg_eps = 0.01 , ** kwargs ) Parameters : pos_eps : The offset in the expression (1-pos_eps)*mu_ap - cos(theta_ap) . See equation 15 in the paper. neg_eps : The offset in the expression cos(theta_an) - (1+neg_eps)*mu_an . See equation 16 in the paper. You should probably use this in conjunction with another loss, as described in the paper. You can accomplish this by using MultipleLosses : main_loss = losses . TupletMarginLoss ( margin = 5 ) var_loss = losses . IntraPairVarianceLoss () complete_loss = losses . MultipleLosses ([ main_loss , var_loss ], weights = [ 1 , 0.5 ]) LargeMarginSoftmaxLoss \u00b6 Large-Margin Softmax Loss for Convolutional Neural Networks losses . LargeMarginSoftmaxLoss ( margin , num_classes , embedding_size , scale = 1 , normalize_weights = False , ** kwargs ) Parameters : margin : An integer which dictates the size of the angular margin. Specifically, it multiplies the angle between the embeddings and weights: cos(margin*theta) . num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) normalize_weights : If True, the learned weights will be normalized to have Euclidean norm of 1, before the loss is computed. Note that when this parameter is True, it becomes equivalent to SphereFaceLoss . Other info : This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . LargeMarginSoftmaxLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () MarginLoss \u00b6 Sampling Matters in Deep Embedding Learning losses . MarginLoss ( margin , nu , beta , triplets_per_anchor = \"all\" , ** kwargs ) Parameters : margin : The radius of the minimalbuffer between positive and negative pairs. nu : The regularization weight for the magnitude of beta. beta : The center of the minimal buffer between positive and negative pairs. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. To make beta a learnable parameter (as done in the paper), pass in the keyword argument: learnable_param_names = [ \"beta\" ] You can then pass the loss function's parameters() to any PyTorch optimizer. MultipleLosses \u00b6 This is a simple wrapper for multiple losses. Pass in a list of already-initialized loss functions. Then, when you call forward on this object, it will return the sum of all wrapped losses. losses . MultipleLosses ( losses , weights = None ) Parameters : losses : A list of initialized loss functions. On the forward call of MultipleLosses, each wrapped loss will be computed, and then the average will be returned. weights : Optional. A list of loss weights, which will be multiplied by the corresponding losses obtained by the loss functions. The default is to multiply each loss by 1. MultiSimilarityLoss \u00b6 Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning losses . MultiSimilarityLoss ( alpha , beta , base = 0.5 , ** kwargs ) Parameters : alpha : The weight applied to positive pairs. beta : The weight applied to negative pairs. base : The offset applied to the exponent in the loss. NCALoss \u00b6 Neighbourhood Components Analysis losses . NCALoss ( softmax_scale = 1 , ** kwargs ) Parameters : softmax_scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) NormalizedSoftmaxLoss \u00b6 Classification is a Strong Baseline for Deep Metric Learning losses . NormalizedSoftmaxLoss ( temperature , embedding_size , num_classes , ** kwargs ) Parameters : temperature : The exponent divisor in the softmax funtion. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. num_classes : The number of classes in your training dataset. Other info This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . NormalizedSoftmaxLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () NPairsLoss \u00b6 Improved Deep Metric Learning with Multi-class N-pair Loss Objective losses . NPairsLoss ( l2_reg_weight = 0 , ** kwargs ) Parameters : l2_reg_weight : The regularization weight for the L2 norm of the embeddings. NTXentLoss \u00b6 This is the normalized temperature-scaled cross entropy loss used in A Simple Framework for Contrastive Learning of Visual Representations . losses . NTXentLoss ( temperature , ** kwargs ) Parameters : temperature : The exponent divisor in the softmax funtion. ProxyAnchorLoss \u00b6 Proxy Anchor Loss for Deep Metric Learning losses . ProxyAnchorLoss ( num_classes , embedding_size , margin = 0.1 , alpha = 32 , ** kwargs ) Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. margin : This is subtracted from the cosine similarity of positive pairs, and added to the cosine similarity of negative pairs. See delta in equation 4 of the paper. alpha : This is the multiplier in the exponent of the LogSumExp expression. See equation 4 in the paper. Other info This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . ProxyAnchorLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () ProxyNCALoss \u00b6 No Fuss Distance Metric Learning using Proxies losses . ProxyNCALoss ( num_classes , embedding_size , ** kwargs ) Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. softmax_scale : See NCALoss Other info This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . ProxyNCALoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () SignalToNoiseRatioContrastiveLoss \u00b6 Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning losses . SignalToNoiseRatioContrastiveLoss ( pos_margin , neg_margin , regularizer_weight , avg_non_zero_only = True , ** kwargs ) Parameters : pos_margin : The noise-to-signal ratio over which positive pairs will contribute to the loss. neg_margin : The noise-to-signal ratio under which negative pairs will contribute to the loss. regularizer_weight : The regularizer encourages the embeddings to have zero-mean distributions. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss. SoftTripleLoss \u00b6 SoftTriple Loss: Deep Metric Learning Without Triplet Sampling losses . SoftTripleLoss ( embedding_size , num_classes , centers_per_class , la = 20 , gamma = 0.1 , reg_weight = 0.2 , margin = 0.01 , ** kwargs ) Parameters : embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. num_classes : The number of classes in your training dataset. centers_per_class : The number of weight vectors per class. (The regular cross entropy loss has 1 center per class.) la : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) gamma : The similarity-to-centers multiplier. reg_weight : The regularization weight which encourages class centers to be close to each other. margin : The margin in the expression e^(similarities - margin). Other info This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . SoftTripleLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () SphereFaceLoss \u00b6 SphereFace: Deep Hypersphere Embedding for Face Recognition losses . SphereFaceLoss ( margin , num_classes , embedding_size , scale = 1 , ** kwargs ) Parameters : margin : An integer which dictates the size of the angular margin. Specifically, it multiplies the angle between the embeddings and weights: cos(margin*theta) . num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) Other info This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . SphereFaceLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step () TripletMarginLoss \u00b6 losses . TripletMarginLoss ( margin = 0.05 , distance_norm = 2 , power = 1 , swap = False , smooth_loss = False , avg_non_zero_only = True , triplets_per_anchor = \"all\" , ** kwargs ) Parameters : margin : The desired difference between the anchor-positive distance and the anchor-negative distance. distance_norm : The norm used when calculating distance between embeddings power : Each pair's loss will be raised to this power. swap : Use the positive-negative distance instead of anchor-negative distance, if it violates the margin more. smooth_loss : Use the log-exp version of the triplet loss avg_non_zero_only : Only triplets that contribute non-zero loss will be used in the final loss. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. TupletMarginLoss \u00b6 Deep Metric Learning with Tuplet Margin Loss losses . TupletMarginLoss ( margin , scale = 64 , ** kwargs ) Parameters : margin : The angular margin (in degrees) applied to positive pairs. The paper uses a value of 5.73 degrees (0.1 radians). scale : The exponent multiplier in the logsumexp expression. The paper combines this loss with IntraPairVarianceLoss . You can accomplish this by using MultipleLosses : main_loss = losses . TupletMarginLoss ( margin = 5 ) var_loss = losses . IntraPairVarianceLoss () complete_loss = losses . MultipleLosses ([ main_loss , var_loss ], weights = [ 1 , 0.5 ]) WeightRegularizerMixin \u00b6 Losses can extend this class in addition to BaseMetricLossFunction. You should extend this class if your loss function can make use of a weight regularizer . losses . WeightRegularizerMixin ( regularizer , reg_weight , ** kwargs ) Parameters : regularizer : The regularizer to apply to the loss's learned weights. reg_weight : The amount the regularization loss will be multiplied by. Extended by: ArcFaceLoss CosFaceLoss LargeMarginSoftmaxLoss NormalizedSoftmaxLoss ProxyAnchorLoss ProxyNCALoss SphereFaceLoss","title":"Losses"},{"location":"losses/#losses","text":"All loss functions are used as follows: from pytorch_metric_learning import losses loss_func = losses . SomeLoss () loss = loss_func ( embeddings , labels ) Or if you are using a loss in conjunction with a miner: from pytorch_metric_learning import miners , losses miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) losses = loss_func ( embeddings , labels , miner_output )","title":"Losses"},{"location":"losses/#angularloss","text":"Deep Metric Learning with Angular Loss losses . AngularLoss ( alpha , ** kwargs ) Parameters : alpha : The angle (as described in the paper), specified in degrees.","title":"AngularLoss"},{"location":"losses/#arcfaceloss","text":"ArcFace: Additive Angular Margin Loss for Deep Face Recognition losses . ArcFaceLoss ( margin , num_classes , embedding_size , scale = 64 , ** kwargs ) Parameters : margin : The angular margin penalty in degrees. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) Other info : This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . ArcFaceLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step ()","title":"ArcFaceLoss"},{"location":"losses/#basemetriclossfunction","text":"All loss functions extend this class and therefore inherit its __init__ parameters. losses . BaseMetricLossFunction ( normalize_embeddings = True , num_class_per_param = None , learnable_param_names = None ) Parameters : normalize_embeddings : If True, embeddings will be normalized to have a Euclidean norm of 1 before the loss is computed. num_class_per_param : If learnable_param_names is set, then this represents the number of classes for each parameter. If your parameters don't have a separate value for each class, then you can leave this at None. learnable_param_names : A list of strings where each element is the name of attributes that should be converted to nn.Parameter. If None, then no parameters are converted. Required Implementations : def compute_loss ( self , embeddings , labels , indices_tuple = None ): raise NotImplementedError","title":"BaseMetricLossFunction"},{"location":"losses/#circleloss","text":"Circle Loss: A Unified Perspective of Pair Similarity Optimization losses . CircleLoss ( m = 0.4 , gamma = 80 , triplets_per_anchor = 'all' , ** kwargs ) Parameters : m : The relaxation factor that controls the radious of the decision boundary. gamma : The scale factor that determines the largest scale of each similarity score. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used.","title":"CircleLoss"},{"location":"losses/#contrastiveloss","text":"losses . ContrastiveLoss ( pos_margin = 0 , neg_margin = 1 , use_similarity = False , power = 1 , avg_non_zero_only = True , ** kwargs ): Parameters : pos_margin : The distance (or similarity) over (under) which positive pairs will contribute to the loss. neg_margin : The distance (or similarity) under (over) which negative pairs will contribute to the loss. use_similarity : If True, will use dot product between vectors instead of euclidean distance. power : Each pair's loss will be raised to this power. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss. Note that the default values for pos_margin and neg_margin are suitable if use_similarity = False . If you set use_similarity = True , then more appropriate values would be pos_margin = 1 and neg_margin = 0 .","title":"ContrastiveLoss"},{"location":"losses/#cosfaceloss","text":"CosFace: Large Margin Cosine Loss for Deep Face Recognition losses . CosFaceLoss ( margin , num_classes , embedding_size , scale = 64 , ** kwargs ) Parameters : margin : The cosine margin penalty: cos(theta) - margin . The paper got optimal performance with margin values between 0.25 and 0.45. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) Other info : This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . CosFaceLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step ()","title":"CosFaceLoss"},{"location":"losses/#crossbatchmemory","text":"This wraps a loss function, and implements Cross-Batch Memory for Embedding Learning . It stores embeddings from previous iterations in a queue, and uses them to form more pairs/triplets with the current iteration's embeddings. losses . CrossBatchMemory ( loss , embedding_size , memory_size = 1024 , miner = None ) Parameters : loss : The loss function to be wrapped. For example, you could pass in ContrastiveLoss() . embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. memory_size : The size of the memory queue. miner : An optional tuple miner , which will be used to mine pairs/triplets from the memory queue.","title":"CrossBatchMemory"},{"location":"losses/#fastaploss","text":"Deep Metric Learning to Rank losses . FastAPLoss ( num_bins , ** kwargs ) Parameters : num_bins : The number of soft histogram bins for calculating average precision","title":"FastAPLoss"},{"location":"losses/#genericpairloss","text":"losses . GenericPairLoss ( use_similarity , mat_based_loss , squared_distances = False , ** kwargs ) Parameters : use_similarity : Set to True if the loss function uses pairwise similarity (dot product of each embedding pair). Otherwise, euclidean distance will be used. mat_based_loss : See required implementations. squared_distances : If True, then the euclidean distance will be squared. Required Implementations : # If mat_based_loss is True, then this takes in mat, pos_mask, and neg_mask # If False, this takes in pos_pairs and neg_pairs def _compute_loss ( self ): raise NotImplementedError","title":"GenericPairLoss"},{"location":"losses/#generalizedliftedstructureloss","text":"losses . GeneralizedLiftedStructureLoss ( neg_margin , ** kwargs ) Parameters : neg_margin : The margin in the expression e^(margin - negative_distance)","title":"GeneralizedLiftedStructureLoss"},{"location":"losses/#intrapairvarianceloss","text":"Deep Metric Learning with Tuplet Margin Loss losses . IntraPairVarianceLoss ( pos_eps = 0.01 , neg_eps = 0.01 , ** kwargs ) Parameters : pos_eps : The offset in the expression (1-pos_eps)*mu_ap - cos(theta_ap) . See equation 15 in the paper. neg_eps : The offset in the expression cos(theta_an) - (1+neg_eps)*mu_an . See equation 16 in the paper. You should probably use this in conjunction with another loss, as described in the paper. You can accomplish this by using MultipleLosses : main_loss = losses . TupletMarginLoss ( margin = 5 ) var_loss = losses . IntraPairVarianceLoss () complete_loss = losses . MultipleLosses ([ main_loss , var_loss ], weights = [ 1 , 0.5 ])","title":"IntraPairVarianceLoss"},{"location":"losses/#largemarginsoftmaxloss","text":"Large-Margin Softmax Loss for Convolutional Neural Networks losses . LargeMarginSoftmaxLoss ( margin , num_classes , embedding_size , scale = 1 , normalize_weights = False , ** kwargs ) Parameters : margin : An integer which dictates the size of the angular margin. Specifically, it multiplies the angle between the embeddings and weights: cos(margin*theta) . num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) normalize_weights : If True, the learned weights will be normalized to have Euclidean norm of 1, before the loss is computed. Note that when this parameter is True, it becomes equivalent to SphereFaceLoss . Other info : This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . LargeMarginSoftmaxLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step ()","title":"LargeMarginSoftmaxLoss"},{"location":"losses/#marginloss","text":"Sampling Matters in Deep Embedding Learning losses . MarginLoss ( margin , nu , beta , triplets_per_anchor = \"all\" , ** kwargs ) Parameters : margin : The radius of the minimalbuffer between positive and negative pairs. nu : The regularization weight for the magnitude of beta. beta : The center of the minimal buffer between positive and negative pairs. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. To make beta a learnable parameter (as done in the paper), pass in the keyword argument: learnable_param_names = [ \"beta\" ] You can then pass the loss function's parameters() to any PyTorch optimizer.","title":"MarginLoss"},{"location":"losses/#multiplelosses","text":"This is a simple wrapper for multiple losses. Pass in a list of already-initialized loss functions. Then, when you call forward on this object, it will return the sum of all wrapped losses. losses . MultipleLosses ( losses , weights = None ) Parameters : losses : A list of initialized loss functions. On the forward call of MultipleLosses, each wrapped loss will be computed, and then the average will be returned. weights : Optional. A list of loss weights, which will be multiplied by the corresponding losses obtained by the loss functions. The default is to multiply each loss by 1.","title":"MultipleLosses"},{"location":"losses/#multisimilarityloss","text":"Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning losses . MultiSimilarityLoss ( alpha , beta , base = 0.5 , ** kwargs ) Parameters : alpha : The weight applied to positive pairs. beta : The weight applied to negative pairs. base : The offset applied to the exponent in the loss.","title":"MultiSimilarityLoss"},{"location":"losses/#ncaloss","text":"Neighbourhood Components Analysis losses . NCALoss ( softmax_scale = 1 , ** kwargs ) Parameters : softmax_scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.)","title":"NCALoss"},{"location":"losses/#normalizedsoftmaxloss","text":"Classification is a Strong Baseline for Deep Metric Learning losses . NormalizedSoftmaxLoss ( temperature , embedding_size , num_classes , ** kwargs ) Parameters : temperature : The exponent divisor in the softmax funtion. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. num_classes : The number of classes in your training dataset. Other info This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . NormalizedSoftmaxLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step ()","title":"NormalizedSoftmaxLoss"},{"location":"losses/#npairsloss","text":"Improved Deep Metric Learning with Multi-class N-pair Loss Objective losses . NPairsLoss ( l2_reg_weight = 0 , ** kwargs ) Parameters : l2_reg_weight : The regularization weight for the L2 norm of the embeddings.","title":"NPairsLoss"},{"location":"losses/#ntxentloss","text":"This is the normalized temperature-scaled cross entropy loss used in A Simple Framework for Contrastive Learning of Visual Representations . losses . NTXentLoss ( temperature , ** kwargs ) Parameters : temperature : The exponent divisor in the softmax funtion.","title":"NTXentLoss"},{"location":"losses/#proxyanchorloss","text":"Proxy Anchor Loss for Deep Metric Learning losses . ProxyAnchorLoss ( num_classes , embedding_size , margin = 0.1 , alpha = 32 , ** kwargs ) Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. margin : This is subtracted from the cosine similarity of positive pairs, and added to the cosine similarity of negative pairs. See delta in equation 4 of the paper. alpha : This is the multiplier in the exponent of the LogSumExp expression. See equation 4 in the paper. Other info This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . ProxyAnchorLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step ()","title":"ProxyAnchorLoss"},{"location":"losses/#proxyncaloss","text":"No Fuss Distance Metric Learning using Proxies losses . ProxyNCALoss ( num_classes , embedding_size , ** kwargs ) Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. softmax_scale : See NCALoss Other info This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . ProxyNCALoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step ()","title":"ProxyNCALoss"},{"location":"losses/#signaltonoiseratiocontrastiveloss","text":"Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning losses . SignalToNoiseRatioContrastiveLoss ( pos_margin , neg_margin , regularizer_weight , avg_non_zero_only = True , ** kwargs ) Parameters : pos_margin : The noise-to-signal ratio over which positive pairs will contribute to the loss. neg_margin : The noise-to-signal ratio under which negative pairs will contribute to the loss. regularizer_weight : The regularizer encourages the embeddings to have zero-mean distributions. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss.","title":"SignalToNoiseRatioContrastiveLoss"},{"location":"losses/#softtripleloss","text":"SoftTriple Loss: Deep Metric Learning Without Triplet Sampling losses . SoftTripleLoss ( embedding_size , num_classes , centers_per_class , la = 20 , gamma = 0.1 , reg_weight = 0.2 , margin = 0.01 , ** kwargs ) Parameters : embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. num_classes : The number of classes in your training dataset. centers_per_class : The number of weight vectors per class. (The regular cross entropy loss has 1 center per class.) la : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) gamma : The similarity-to-centers multiplier. reg_weight : The regularization weight which encourages class centers to be close to each other. margin : The margin in the expression e^(similarities - margin). Other info This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . SoftTripleLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step ()","title":"SoftTripleLoss"},{"location":"losses/#spherefaceloss","text":"SphereFace: Deep Hypersphere Embedding for Face Recognition losses . SphereFaceLoss ( margin , num_classes , embedding_size , scale = 1 , ** kwargs ) Parameters : margin : An integer which dictates the size of the angular margin. Specifically, it multiplies the angle between the embeddings and weights: cos(margin*theta) . num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) Other info This also extends WeightRegularizerMixin , so it accepts a regularizer and reg_weight as optional init arguments. This loss requires an optimizer . You need to create an optimizer and pass this loss's parameters to that optimizer. For example: loss_func = losses . SphereFaceLoss ( ... ) loss_optimizer = torch . optim . SGD ( loss_func . parameters (), lr = 0.01 ) # then during training: loss_optimizer . step ()","title":"SphereFaceLoss"},{"location":"losses/#tripletmarginloss","text":"losses . TripletMarginLoss ( margin = 0.05 , distance_norm = 2 , power = 1 , swap = False , smooth_loss = False , avg_non_zero_only = True , triplets_per_anchor = \"all\" , ** kwargs ) Parameters : margin : The desired difference between the anchor-positive distance and the anchor-negative distance. distance_norm : The norm used when calculating distance between embeddings power : Each pair's loss will be raised to this power. swap : Use the positive-negative distance instead of anchor-negative distance, if it violates the margin more. smooth_loss : Use the log-exp version of the triplet loss avg_non_zero_only : Only triplets that contribute non-zero loss will be used in the final loss. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used.","title":"TripletMarginLoss"},{"location":"losses/#tupletmarginloss","text":"Deep Metric Learning with Tuplet Margin Loss losses . TupletMarginLoss ( margin , scale = 64 , ** kwargs ) Parameters : margin : The angular margin (in degrees) applied to positive pairs. The paper uses a value of 5.73 degrees (0.1 radians). scale : The exponent multiplier in the logsumexp expression. The paper combines this loss with IntraPairVarianceLoss . You can accomplish this by using MultipleLosses : main_loss = losses . TupletMarginLoss ( margin = 5 ) var_loss = losses . IntraPairVarianceLoss () complete_loss = losses . MultipleLosses ([ main_loss , var_loss ], weights = [ 1 , 0.5 ])","title":"TupletMarginLoss"},{"location":"losses/#weightregularizermixin","text":"Losses can extend this class in addition to BaseMetricLossFunction. You should extend this class if your loss function can make use of a weight regularizer . losses . WeightRegularizerMixin ( regularizer , reg_weight , ** kwargs ) Parameters : regularizer : The regularizer to apply to the loss's learned weights. reg_weight : The amount the regularization loss will be multiplied by. Extended by: ArcFaceLoss CosFaceLoss LargeMarginSoftmaxLoss NormalizedSoftmaxLoss ProxyAnchorLoss ProxyNCALoss SphereFaceLoss","title":"WeightRegularizerMixin"},{"location":"miners/","text":"Miners \u00b6 Mining functions come in two flavors: Subset Batch Miners take a batch of N embeddings and return a subset n to be used by a tuple miner, or directly by a loss function. Without a subset batch miner, n == N . Tuple Miners take a batch of n embeddings and return k pairs/triplets to be used for calculating the loss: Pair miners output a tuple of size 4: (anchors, positives, anchors, negatives). Triplet miners output a tuple of size 3: (anchors, positives, negatives). Without a tuple miner, loss functions will by default use all possible pairs/triplets in the batch. Almost all current miners are tuple miners. You might be familiar with the terminology: \"online\" and \"offline\" miners. Tuple miners are online, while subset batch miners are a mix between online and offline. Completely offline miners should be implemented as a PyTorch Sampler . Tuple miners are used with loss functions as follows: from pytorch_metric_learning import miners , losses miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) losses = loss_func ( embeddings , labels , miner_output ) AngularMiner \u00b6 miners . AngularMiner ( angle , ** kwargs ) Parameters angle : The miner will return triplets that form an angle greater than this input angle. The angle is computed as defined in the angular loss paper BaseMiner \u00b6 All miners extend this class and therefore inherit its __init__ parameters. miners . BaseMiner ( normalize_embeddings = True ) Parameters normalize_embeddings : If True, embeddings will be normalized to have a Euclidean norm of 1 before any mining occurs. Required Implementations : # Return indices of some form def mine ( self , embeddings , labels , ref_emb , ref_labels ): raise NotImplementedError Note: by default, embeddings == ref_emb and labels == ref_labels . # Validate the output of the miner. def output_assertion ( self , output ): raise NotImplementedError BaseTupleMiner \u00b6 This extends BaseMiner , and most miners extend this class. It outputs a tuple of indices: Pair miners output a tuple of size 4: (anchors, positives, anchors, negatives) Triplet miners output a tuple of size 3: (anchors, positives, negatives) miners . BaseTupleMiner ( ** kwargs ) If you write your own miner, the mine function should work such that anchor indices correspond to embeddings and labels , and all other indices correspond to ref_emb and ref_labels . By default, embeddings == ref_emb and labels == ref_labels , but separating the anchor source from the positive/negative source allows for interesting use cases. For example, see CrossBatchMemory . BaseSubsetBatchMiner \u00b6 This extends BaseMiner . It outputs indices corresponding to a subset of the input batch. The idea is to use these miners with torch.no_grad(), and with a large input batch size. miners . BaseSubsetBatchMiner ( output_batch_size , ** kwargs ) Parameters output_batch_size : An integer that is the size of the subset that the miner will output. BatchHardMiner \u00b6 In Defense of the Triplet Loss for Person Re-Identification miners . BatchHardMiner ( use_similarity = False , squared_distances = False , ** kwargs ) Parameters use_similarity : If True, will use dot product between vectors instead of euclidean distance. squared_distances : If True, then the euclidean distance will be squared. DistanceWeightedMiner \u00b6 Sampling Matters in Deep Embedding Learning miners . DistanceWeightedMiner ( cutoff , nonzero_loss_cutoff , ** kwargs ) Parameters cutoff : Pairwise distances are clipped to this value if they fall below it. nonzero_loss_cutoff : Pairs that have distance greater than this are discarded. EmbeddingsAlreadyPackagedAsTriplets \u00b6 If your embeddings are already ordered sequentially as triplets, then use this miner to force your loss function to use the already-formed triplets. miners . EmbeddingsAlreadyPackagedAsTriplets () HDCMiner \u00b6 Hard-Aware Deeply Cascaded Embedding miners . HDCMiner ( filter_percentage , use_similarity = False , squared_distances = False , ** kwargs ) Parameters : filter_percentage : The percentage of pairs that will be returned. For example, if filter_percentage is 0.25, then the hardest 25% of pairs will be returned. The pool of pairs is either externally or internally set. See the important methods below for details. use_similarity : If True, will use dot product between vectors instead of euclidean distance. squared_distances : If True, then the euclidean distance will be squared. Important methods : # Pairs or triplets extracted from another miner, # and then passed in to HDCMiner using this function def set_idx_externally ( self , external_indices_tuple , labels ): self . a1 , self . p , self . a2 , self . n = lmu . convert_to_pairs ( external_indices_tuple , labels ) self . was_set_externally = True # Reset the internal state of the HDCMiner def reset_idx ( self ): self . a1 , self . p , self . a2 , self . n = None , None , None , None self . was_set_externally = False Example of passing another miner output to HDCMiner : minerA = miners . MultiSimilarityMiner ( epsilon = 0.1 ) minerB = miners . HDCMiner ( filter_percentage = 0.25 ) hard_pairs = minerA ( embeddings , labels ) minerB . set_idx_externally ( hard_pairs , labels ) very_hard_pairs = minerB ( embeddings , labels ) MaximumLossMiner \u00b6 This is a simple subset batch miner . It computes the loss for random subsets of the input batch, num_trials times. Then it returns the subset with the highest loss. miners . MaximumLossMiner ( loss , miner = None , num_trials = 5 , ** kwargs ) Parameters loss : The loss function used to compute the loss. miner : Optional tuple miner which extracts pairs/triplets for the loss function. num_trials : The number of random subsets to try. MultiSimilarityMiner \u00b6 Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning miners . MultiSimilarityMiner ( epsilon , ** kwargs ) Parameters epsilon : Negative pairs are chosen if they have similarity greater than the hardest positive pair, minus this margin (epsilon). Positive pairs are chosen if they have similarity less than the hardest negative pair, plus this margin (epsilon). PairMarginMiner \u00b6 Returns positive and negative pairs that violate the specified margins. miners . PairMarginMiner ( pos_margin , neg_margin , use_similarity , squared_distances = False , ** kwargs ) Parameters pos_margin : The distance (or similarity) over (under) which positive pairs will be chosen. neg_margin : The distance (or similarity) under (over) which negative pairs will be chosen. use_similarity : If True, will use dot product between vectors instead of euclidean distance. squared_distances : If True, then the euclidean distance will be squared. TripletMarginMiner \u00b6 Returns hard, semihard, or all triplets. miners . TripletMarginMiner ( margin , type_of_triplets = \"all\" , ** kwargs ) Parameters margin : The difference between the anchor-positive distance and the anchor-negative distance. type_of_triplets : \"all\" means all triplets that violate the margin \"hard\" is a subset of \"all\", but the negative is closer to the anchor than the positive \"semihard\" is a subset of \"all\", but the negative is further from the anchor than the positive","title":"Miners"},{"location":"miners/#miners","text":"Mining functions come in two flavors: Subset Batch Miners take a batch of N embeddings and return a subset n to be used by a tuple miner, or directly by a loss function. Without a subset batch miner, n == N . Tuple Miners take a batch of n embeddings and return k pairs/triplets to be used for calculating the loss: Pair miners output a tuple of size 4: (anchors, positives, anchors, negatives). Triplet miners output a tuple of size 3: (anchors, positives, negatives). Without a tuple miner, loss functions will by default use all possible pairs/triplets in the batch. Almost all current miners are tuple miners. You might be familiar with the terminology: \"online\" and \"offline\" miners. Tuple miners are online, while subset batch miners are a mix between online and offline. Completely offline miners should be implemented as a PyTorch Sampler . Tuple miners are used with loss functions as follows: from pytorch_metric_learning import miners , losses miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) losses = loss_func ( embeddings , labels , miner_output )","title":"Miners"},{"location":"miners/#angularminer","text":"miners . AngularMiner ( angle , ** kwargs ) Parameters angle : The miner will return triplets that form an angle greater than this input angle. The angle is computed as defined in the angular loss paper","title":"AngularMiner"},{"location":"miners/#baseminer","text":"All miners extend this class and therefore inherit its __init__ parameters. miners . BaseMiner ( normalize_embeddings = True ) Parameters normalize_embeddings : If True, embeddings will be normalized to have a Euclidean norm of 1 before any mining occurs. Required Implementations : # Return indices of some form def mine ( self , embeddings , labels , ref_emb , ref_labels ): raise NotImplementedError Note: by default, embeddings == ref_emb and labels == ref_labels . # Validate the output of the miner. def output_assertion ( self , output ): raise NotImplementedError","title":"BaseMiner"},{"location":"miners/#basetupleminer","text":"This extends BaseMiner , and most miners extend this class. It outputs a tuple of indices: Pair miners output a tuple of size 4: (anchors, positives, anchors, negatives) Triplet miners output a tuple of size 3: (anchors, positives, negatives) miners . BaseTupleMiner ( ** kwargs ) If you write your own miner, the mine function should work such that anchor indices correspond to embeddings and labels , and all other indices correspond to ref_emb and ref_labels . By default, embeddings == ref_emb and labels == ref_labels , but separating the anchor source from the positive/negative source allows for interesting use cases. For example, see CrossBatchMemory .","title":"BaseTupleMiner"},{"location":"miners/#basesubsetbatchminer","text":"This extends BaseMiner . It outputs indices corresponding to a subset of the input batch. The idea is to use these miners with torch.no_grad(), and with a large input batch size. miners . BaseSubsetBatchMiner ( output_batch_size , ** kwargs ) Parameters output_batch_size : An integer that is the size of the subset that the miner will output.","title":"BaseSubsetBatchMiner"},{"location":"miners/#batchhardminer","text":"In Defense of the Triplet Loss for Person Re-Identification miners . BatchHardMiner ( use_similarity = False , squared_distances = False , ** kwargs ) Parameters use_similarity : If True, will use dot product between vectors instead of euclidean distance. squared_distances : If True, then the euclidean distance will be squared.","title":"BatchHardMiner"},{"location":"miners/#distanceweightedminer","text":"Sampling Matters in Deep Embedding Learning miners . DistanceWeightedMiner ( cutoff , nonzero_loss_cutoff , ** kwargs ) Parameters cutoff : Pairwise distances are clipped to this value if they fall below it. nonzero_loss_cutoff : Pairs that have distance greater than this are discarded.","title":"DistanceWeightedMiner"},{"location":"miners/#embeddingsalreadypackagedastriplets","text":"If your embeddings are already ordered sequentially as triplets, then use this miner to force your loss function to use the already-formed triplets. miners . EmbeddingsAlreadyPackagedAsTriplets ()","title":"EmbeddingsAlreadyPackagedAsTriplets"},{"location":"miners/#hdcminer","text":"Hard-Aware Deeply Cascaded Embedding miners . HDCMiner ( filter_percentage , use_similarity = False , squared_distances = False , ** kwargs ) Parameters : filter_percentage : The percentage of pairs that will be returned. For example, if filter_percentage is 0.25, then the hardest 25% of pairs will be returned. The pool of pairs is either externally or internally set. See the important methods below for details. use_similarity : If True, will use dot product between vectors instead of euclidean distance. squared_distances : If True, then the euclidean distance will be squared. Important methods : # Pairs or triplets extracted from another miner, # and then passed in to HDCMiner using this function def set_idx_externally ( self , external_indices_tuple , labels ): self . a1 , self . p , self . a2 , self . n = lmu . convert_to_pairs ( external_indices_tuple , labels ) self . was_set_externally = True # Reset the internal state of the HDCMiner def reset_idx ( self ): self . a1 , self . p , self . a2 , self . n = None , None , None , None self . was_set_externally = False Example of passing another miner output to HDCMiner : minerA = miners . MultiSimilarityMiner ( epsilon = 0.1 ) minerB = miners . HDCMiner ( filter_percentage = 0.25 ) hard_pairs = minerA ( embeddings , labels ) minerB . set_idx_externally ( hard_pairs , labels ) very_hard_pairs = minerB ( embeddings , labels )","title":"HDCMiner"},{"location":"miners/#maximumlossminer","text":"This is a simple subset batch miner . It computes the loss for random subsets of the input batch, num_trials times. Then it returns the subset with the highest loss. miners . MaximumLossMiner ( loss , miner = None , num_trials = 5 , ** kwargs ) Parameters loss : The loss function used to compute the loss. miner : Optional tuple miner which extracts pairs/triplets for the loss function. num_trials : The number of random subsets to try.","title":"MaximumLossMiner"},{"location":"miners/#multisimilarityminer","text":"Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning miners . MultiSimilarityMiner ( epsilon , ** kwargs ) Parameters epsilon : Negative pairs are chosen if they have similarity greater than the hardest positive pair, minus this margin (epsilon). Positive pairs are chosen if they have similarity less than the hardest negative pair, plus this margin (epsilon).","title":"MultiSimilarityMiner"},{"location":"miners/#pairmarginminer","text":"Returns positive and negative pairs that violate the specified margins. miners . PairMarginMiner ( pos_margin , neg_margin , use_similarity , squared_distances = False , ** kwargs ) Parameters pos_margin : The distance (or similarity) over (under) which positive pairs will be chosen. neg_margin : The distance (or similarity) under (over) which negative pairs will be chosen. use_similarity : If True, will use dot product between vectors instead of euclidean distance. squared_distances : If True, then the euclidean distance will be squared.","title":"PairMarginMiner"},{"location":"miners/#tripletmarginminer","text":"Returns hard, semihard, or all triplets. miners . TripletMarginMiner ( margin , type_of_triplets = \"all\" , ** kwargs ) Parameters margin : The difference between the anchor-positive distance and the anchor-negative distance. type_of_triplets : \"all\" means all triplets that violate the margin \"hard\" is a subset of \"all\", but the negative is closer to the anchor than the positive \"semihard\" is a subset of \"all\", but the negative is further from the anchor than the positive","title":"TripletMarginMiner"},{"location":"regularizers/","text":"Regularizers \u00b6 Regularizers are like helper-loss functions. They might not require any labels or embeddings as input, and might instead operate on weights that are learned by a network or loss function. Here is an example when used in conjunction with a compatible loss function: from pytorch_metric_learning import losses , regularizers R = regularizers . RegularFaceRegularizer () loss = losses . ArcFaceLoss ( margin = 30 , num_classes = 100 , embedding_size = 128 , regularizer = R ) BaseWeightRegularizer \u00b6 Weight regularizers take in a 2-D tensor of weights of size (num_classes, embedding_size). regularizers . BaseWeightRegularizer ( normalize_weights = True ) Parameters normalize_weights : If True, weights will be normalized to have a Euclidean norm of 1 before any regularization occurs. An object of this class can be passed as the regularizer argument into any class that extends WeightRegularizerMixin . CenterInvariantRegularizer \u00b6 Deep Face Recognition with Center Invariant Loss regularizers . CenterInvariantRegularizer ( normalize_weights = False ) Extends BaseWeightRegularizer . normalize_weights must be False. RegularFaceRegularizer \u00b6 RegularFace: Deep Face Recognition via Exclusive Regularization regularizers . RegularFaceRegularizer ( normalize_weights = True ) Extends BaseWeightRegularizer .","title":"Regularizers"},{"location":"regularizers/#regularizers","text":"Regularizers are like helper-loss functions. They might not require any labels or embeddings as input, and might instead operate on weights that are learned by a network or loss function. Here is an example when used in conjunction with a compatible loss function: from pytorch_metric_learning import losses , regularizers R = regularizers . RegularFaceRegularizer () loss = losses . ArcFaceLoss ( margin = 30 , num_classes = 100 , embedding_size = 128 , regularizer = R )","title":"Regularizers"},{"location":"regularizers/#baseweightregularizer","text":"Weight regularizers take in a 2-D tensor of weights of size (num_classes, embedding_size). regularizers . BaseWeightRegularizer ( normalize_weights = True ) Parameters normalize_weights : If True, weights will be normalized to have a Euclidean norm of 1 before any regularization occurs. An object of this class can be passed as the regularizer argument into any class that extends WeightRegularizerMixin .","title":"BaseWeightRegularizer"},{"location":"regularizers/#centerinvariantregularizer","text":"Deep Face Recognition with Center Invariant Loss regularizers . CenterInvariantRegularizer ( normalize_weights = False ) Extends BaseWeightRegularizer . normalize_weights must be False.","title":"CenterInvariantRegularizer"},{"location":"regularizers/#regularfaceregularizer","text":"RegularFace: Deep Face Recognition via Exclusive Regularization regularizers . RegularFaceRegularizer ( normalize_weights = True ) Extends BaseWeightRegularizer .","title":"RegularFaceRegularizer"},{"location":"samplers/","text":"Samplers \u00b6 Samplers are just extensions of the torch.utils.data.Sampler class, i.e. they are passed to a PyTorch Dataloader. The purpose of samplers is to determine how batches should be formed. This is also where any offline pair or triplet miners should exist. MPerClassSampler \u00b6 At every iteration, this will return m samples per class, assuming that the batch size is a multiple of m . For example, if your dataloader's batch size is 100, and m = 5, then 20 classes with 5 samples each will be returned. samplers . MPerClassSampler ( labels , m ) Parameters : labels : The list of labels for your dataset, i.e. the labels[x] should be the label of the xth element in your dataset. m : The number of samples per class to fetch at every iteration. If a class has less than m samples, then there will be duplicates in the returned batch. FixedSetOfTriplets \u00b6 When initialized, this class creates a fixed set of triplets. This is useful for determining the performance of algorithms in cases where the only ground truth data is a set of triplets. samplers . FixedSetOfTriplets ( labels , num_triplets ) Parameters : labels : The list of labels for your dataset, i.e. the labels[x] should be the label of the xth element in your dataset. num_triplets : The number of triplets to create.","title":"Samplers"},{"location":"samplers/#samplers","text":"Samplers are just extensions of the torch.utils.data.Sampler class, i.e. they are passed to a PyTorch Dataloader. The purpose of samplers is to determine how batches should be formed. This is also where any offline pair or triplet miners should exist.","title":"Samplers"},{"location":"samplers/#mperclasssampler","text":"At every iteration, this will return m samples per class, assuming that the batch size is a multiple of m . For example, if your dataloader's batch size is 100, and m = 5, then 20 classes with 5 samples each will be returned. samplers . MPerClassSampler ( labels , m ) Parameters : labels : The list of labels for your dataset, i.e. the labels[x] should be the label of the xth element in your dataset. m : The number of samples per class to fetch at every iteration. If a class has less than m samples, then there will be duplicates in the returned batch.","title":"MPerClassSampler"},{"location":"samplers/#fixedsetoftriplets","text":"When initialized, this class creates a fixed set of triplets. This is useful for determining the performance of algorithms in cases where the only ground truth data is a set of triplets. samplers . FixedSetOfTriplets ( labels , num_triplets ) Parameters : labels : The list of labels for your dataset, i.e. the labels[x] should be the label of the xth element in your dataset. num_triplets : The number of triplets to create.","title":"FixedSetOfTriplets"},{"location":"testers/","text":"Testers \u00b6 Testers take your model and dataset, and compute nearest-neighbor based accuracy metrics. Note that the testers require the faiss package , which you can install with conda. In general, testers are used as follows: from pytorch_metric_learning import testers t = testers . SomeTestingFunction ( * args , ** kwargs ) dataset_dict = { \"train\" : train_dataset , \"val\" : val_dataset } tester . test ( dataset_dict , epoch , model ) # Or if your model is composed of a trunk + embedder tester . test ( dataset_dict , epoch , trunk , embedder ) BaseTester \u00b6 All trainers extend this class and therefore inherit its __init__ arguments. testers . BaseTester ( reference_set = \"compared_to_self\" , normalize_embeddings = True , use_trunk_output = False , batch_size = 32 , dataloader_num_workers = 32 , pca = None , data_device = None , data_and_label_getter = None , label_hierarchy_level = 0 , end_of_testing_hook = None , dataset_labels = None , dataset_labels = None , set_min_label_to_zero = False , accuracy_calculator = None , visualizer = None , visualizer_hook = None ) Parameters : reference_set : Must be one of the following: \"compared_to_self\": each dataset split will refer to itself to find nearest neighbors. \"compared_to_sets_combined\": each dataset split will refer to all provided splits to find nearest neighbors. \"compared_to_training_set\": each dataset will refer to the training set to find nearest neighbors. normalize_embeddings : If True, embeddings will be normalized to Euclidean norm of 1 before nearest neighbors are computed. use_trunk_output : If True, the output of the trunk_model will be used to compute nearest neighbors, i.e. the output of the embedder model will be ignored. batch_size : How many dataset samples to process at each iteration when computing embeddings. dataloader_num_workers : How many processes the dataloader will use. pca : The number of dimensions that your embeddings will be reduced to, using PCA. The default is None, meaning PCA will not be applied. data_device : Which gpu to use for the loaded dataset samples. If None, then the gpu or cpu will be used (whichever is available). data_and_label_getter : A function that takes the output of your dataset's __getitem__ function, and returns a tuple of (data, labels). If None, then it is assumed that __getitem__ returns (data, labels). label_hierarchy_level : If each sample in your dataset has multiple labels, then this integer argument can be used to select which \"level\" to use. This assumes that your labels are \"2-dimensional\" with shape (num_samples, num_hierarchy_levels). Leave this at the default value, 0, if your data does not have multiple labels per sample. end_of_testing_hook : This is an optional function that has one input argument (the tester object) and performs some action (e.g. logging data) at the end of testing. You'll probably want to access the accuracy metrics, which are stored in tester.all_accuracies . This is a nested dictionary with the following format: tester.all_accuracies[split_name][metric_name] = metric_value If you set size_of_tsne to be greater than 0, then the T-SNE embeddings will be stored in tester.tsne_embeddings which is a dictionary with the following format: tester.tsne_embeddings[split_name][\"tsne_level%d\"] = (embeddings, labels) . (Note that \"tsne_level%d\" refers to the label hierarchy level. If you use the default label hierarchy level, then the string will be \"tsne_level0\" .) If you want ready-to-use hooks, take a look at the logging_presets module . dataset_labels : The labels for your dataset. Can be 1-dimensional (1 label per datapoint) or 2-dimensional, where each row represents a datapoint, and the columns are the multiple labels that the datapoint has. Labels can be integers or strings. This option needs to be specified only if set_min_label_to_zero is True. set_min_label_to_zero : If True, labels will be mapped such that they represent their rank in the label set. For example, if your dataset has labels 5, 10, 12, 13, then at each iteration, these would become 0, 1, 2, 3. You should also set this to True if you want to use string labels. In that case, 'dog', 'cat', 'monkey' would get mapped to 1, 0, 2. If True, you must pass in dataset_labels (see above). The default is False. accuracy_calculator : Optional. An object that extends AccuracyCalculator . This will be used to compute the accuracy of your model. By default, AccuracyCalculator is used. visualizer : Optional. An object that has implemented the fit_transform method, as done by UMAP and many scikit-learn functions. For example, you can set visualizer = umap.UMAP() . The object's fit_transform function should take in a 2D array of embeddings, and reduce the dimensionality, such that calling visualizer.fit_transform(embeddings) results in a 2D array of size (N, 2). visualizer_hook : Optional. This function will be passed the following args. You can do whatever you want in this function, but the reason it exists is to allow you to save a plot of the embeddings etc. visualizer: The visualizer object that you passed in. embeddings: The dimensionality reduced embeddings. label: The corresponding labels for each embedding. split_name: The name of the split (train, val, etc.) keyname: The name of the dictionary key where the embeddings and labels are stored. (The dictionary is self.dim_reduced_embeddings[split_name][keyname].) epoch: The epoch for which the embeddings are being computed. GlobalEmbeddingSpaceTester \u00b6 Computes nearest neighbors by looking at all points in the embedding space. This is probably the tester you are looking for. To see it in action, check one of the example notebooks testers . GlobalEmbeddingSpaceTester ( * args , ** kwargs ) WithSameParentLabelTester \u00b6 This assumes there is a label hierarchy. For each sample, the search space is narrowed by only looking at sibling samples, i.e. samples with the same parent label. For example, consider a dataset with 4 fine-grained classes {cat, dog, car, truck}, and 2 coarse-grained classes {animal, vehicle}. The nearest neighbor search for cats and dogs will consist of animals, and the nearest-neighbor search for cars and trucks will consist of vehicles. testers . WithSameParentLabelTester ( * args , ** kwargs ) GlobalTwoStreamEmbeddingSpaceTester \u00b6 This is the corresponding tester for TwoStreamMetricLoss . The supplied dataset must return (anchor, positive, label) . testers . GlobalTwoStreamEmbeddingSpaceTester ( * args , ** kwargs ) Requirements : reference_set : This must be left at the default value of \"compared_to_self\" .","title":"Testers"},{"location":"testers/#testers","text":"Testers take your model and dataset, and compute nearest-neighbor based accuracy metrics. Note that the testers require the faiss package , which you can install with conda. In general, testers are used as follows: from pytorch_metric_learning import testers t = testers . SomeTestingFunction ( * args , ** kwargs ) dataset_dict = { \"train\" : train_dataset , \"val\" : val_dataset } tester . test ( dataset_dict , epoch , model ) # Or if your model is composed of a trunk + embedder tester . test ( dataset_dict , epoch , trunk , embedder )","title":"Testers"},{"location":"testers/#basetester","text":"All trainers extend this class and therefore inherit its __init__ arguments. testers . BaseTester ( reference_set = \"compared_to_self\" , normalize_embeddings = True , use_trunk_output = False , batch_size = 32 , dataloader_num_workers = 32 , pca = None , data_device = None , data_and_label_getter = None , label_hierarchy_level = 0 , end_of_testing_hook = None , dataset_labels = None , dataset_labels = None , set_min_label_to_zero = False , accuracy_calculator = None , visualizer = None , visualizer_hook = None ) Parameters : reference_set : Must be one of the following: \"compared_to_self\": each dataset split will refer to itself to find nearest neighbors. \"compared_to_sets_combined\": each dataset split will refer to all provided splits to find nearest neighbors. \"compared_to_training_set\": each dataset will refer to the training set to find nearest neighbors. normalize_embeddings : If True, embeddings will be normalized to Euclidean norm of 1 before nearest neighbors are computed. use_trunk_output : If True, the output of the trunk_model will be used to compute nearest neighbors, i.e. the output of the embedder model will be ignored. batch_size : How many dataset samples to process at each iteration when computing embeddings. dataloader_num_workers : How many processes the dataloader will use. pca : The number of dimensions that your embeddings will be reduced to, using PCA. The default is None, meaning PCA will not be applied. data_device : Which gpu to use for the loaded dataset samples. If None, then the gpu or cpu will be used (whichever is available). data_and_label_getter : A function that takes the output of your dataset's __getitem__ function, and returns a tuple of (data, labels). If None, then it is assumed that __getitem__ returns (data, labels). label_hierarchy_level : If each sample in your dataset has multiple labels, then this integer argument can be used to select which \"level\" to use. This assumes that your labels are \"2-dimensional\" with shape (num_samples, num_hierarchy_levels). Leave this at the default value, 0, if your data does not have multiple labels per sample. end_of_testing_hook : This is an optional function that has one input argument (the tester object) and performs some action (e.g. logging data) at the end of testing. You'll probably want to access the accuracy metrics, which are stored in tester.all_accuracies . This is a nested dictionary with the following format: tester.all_accuracies[split_name][metric_name] = metric_value If you set size_of_tsne to be greater than 0, then the T-SNE embeddings will be stored in tester.tsne_embeddings which is a dictionary with the following format: tester.tsne_embeddings[split_name][\"tsne_level%d\"] = (embeddings, labels) . (Note that \"tsne_level%d\" refers to the label hierarchy level. If you use the default label hierarchy level, then the string will be \"tsne_level0\" .) If you want ready-to-use hooks, take a look at the logging_presets module . dataset_labels : The labels for your dataset. Can be 1-dimensional (1 label per datapoint) or 2-dimensional, where each row represents a datapoint, and the columns are the multiple labels that the datapoint has. Labels can be integers or strings. This option needs to be specified only if set_min_label_to_zero is True. set_min_label_to_zero : If True, labels will be mapped such that they represent their rank in the label set. For example, if your dataset has labels 5, 10, 12, 13, then at each iteration, these would become 0, 1, 2, 3. You should also set this to True if you want to use string labels. In that case, 'dog', 'cat', 'monkey' would get mapped to 1, 0, 2. If True, you must pass in dataset_labels (see above). The default is False. accuracy_calculator : Optional. An object that extends AccuracyCalculator . This will be used to compute the accuracy of your model. By default, AccuracyCalculator is used. visualizer : Optional. An object that has implemented the fit_transform method, as done by UMAP and many scikit-learn functions. For example, you can set visualizer = umap.UMAP() . The object's fit_transform function should take in a 2D array of embeddings, and reduce the dimensionality, such that calling visualizer.fit_transform(embeddings) results in a 2D array of size (N, 2). visualizer_hook : Optional. This function will be passed the following args. You can do whatever you want in this function, but the reason it exists is to allow you to save a plot of the embeddings etc. visualizer: The visualizer object that you passed in. embeddings: The dimensionality reduced embeddings. label: The corresponding labels for each embedding. split_name: The name of the split (train, val, etc.) keyname: The name of the dictionary key where the embeddings and labels are stored. (The dictionary is self.dim_reduced_embeddings[split_name][keyname].) epoch: The epoch for which the embeddings are being computed.","title":"BaseTester"},{"location":"testers/#globalembeddingspacetester","text":"Computes nearest neighbors by looking at all points in the embedding space. This is probably the tester you are looking for. To see it in action, check one of the example notebooks testers . GlobalEmbeddingSpaceTester ( * args , ** kwargs )","title":"GlobalEmbeddingSpaceTester"},{"location":"testers/#withsameparentlabeltester","text":"This assumes there is a label hierarchy. For each sample, the search space is narrowed by only looking at sibling samples, i.e. samples with the same parent label. For example, consider a dataset with 4 fine-grained classes {cat, dog, car, truck}, and 2 coarse-grained classes {animal, vehicle}. The nearest neighbor search for cats and dogs will consist of animals, and the nearest-neighbor search for cars and trucks will consist of vehicles. testers . WithSameParentLabelTester ( * args , ** kwargs )","title":"WithSameParentLabelTester"},{"location":"testers/#globaltwostreamembeddingspacetester","text":"This is the corresponding tester for TwoStreamMetricLoss . The supplied dataset must return (anchor, positive, label) . testers . GlobalTwoStreamEmbeddingSpaceTester ( * args , ** kwargs ) Requirements : reference_set : This must be left at the default value of \"compared_to_self\" .","title":"GlobalTwoStreamEmbeddingSpaceTester"},{"location":"trainers/","text":"Trainers \u00b6 Trainers exist in this library because some metric learning algorithms are more than just loss or mining functions. Some algorithms require additional networks, data augmentations, learning rate schedules etc. The goal of the trainers module is to provide access to these type of metric learning algorithms. In general, trainers are used as follows: from pytorch_metric_learning import trainers t = trainers . SomeTrainingFunction ( * args , ** kwargs ) t . train ( num_epochs = 10 ) BaseTrainer \u00b6 All trainers extend this class and therefore inherit its __init__ arguments. trainers . BaseTrainer ( models , optimizers , batch_size , loss_funcs , mining_funcs , dataset , iterations_per_epoch = None , data_device = None , loss_weights = None , sampler = None , collate_fn = None , lr_schedulers = None , gradient_clippers = None , freeze_these = (), freeze_trunk_batchnorm = False , label_hierarchy_level = 0 , dataloader_num_workers = 32 , data_and_label_getter = None , dataset_labels = None , set_min_label_to_zero = False , end_of_iteration_hook = None , end_of_epoch_hook = None ) Parameters : models : A dictionary of the form: {\"trunk\": trunk_model, \"embedder\": embedder_model} The \"embedder\" key is optional. optimizers : A dictionary mapping strings to optimizers. The base class does not require any specific keys. For example, you could provide an empty dictionary, in which case no optimization will happen. Or you could provide just an optimizer for your trunk_model. But most likely, you'll want to pass in: {\"trunk_optimizer\": trunk_optimizer, \"embedder_optimizer\": embedder_optimizer}. batch_size : The number of elements that are retrieved at each iteration. loss_funcs : A dictionary mapping strings to loss functions. The required keys depend on the training method, but all methods are likely to require at least: {\"metric_loss\": loss_func}. mining_funcs : A dictionary mapping strings to mining functions. Pass in an empty dictionary, or one or more of the following keys: {\"subset_batch_miner\": mining_func1, \"tuple_miner\": mining_func2} dataset : The dataset you want to train on. Note that training methods do not perform validation, so do not pass in your validation or test set. data_device : The device that you want to put batches of data on. If not specified, the trainer will put the data on any available GPUs. iterations_per_epoch : Optional. If you don't specify iterations_per_epoch : 1 epoch = 1 pass through the dataloader iterator. If sampler=None , then 1 pass through the iterator is 1 pass through the dataset. If you use a sampler, then 1 pass through the iterator is 1 pass through the iterable returned by the sampler. For samplers like MPerClassSampler or some offline mining method, the iterable returned might be very long or very short etc, and might not be related to the length of the dataset. The length of the epoch might vary each time the sampler creates a new iterable. In these cases, it can be useful to specify iterations_per_epoch so that each \"epoch\" is just a fixed number of iterations. The definition of epoch matters because there's various things like LR schedulers and hooks that depend on an epoch ending. loss_weights : A dictionary mapping loss names to numbers. Each loss will be multiplied by the corresponding value in the dictionary. If not specified, then no loss weighting will occur. If not specified, then the original labels are used. sampler : The sampler used by the dataloader. If not specified, then random sampling will be used. collate_fn : The collate function used by the dataloader. lr_scheduers : A dictionary of PyTorch learning rate schedulers. Your keys should be strings of the form <model>_<step_type> , where <model> is a key that comes from either the models or loss_funcs dictionary, and <step_type> is one of the following: \"scheduler_by_iteration\" (will be stepped at every iteration) \"scheduler_by_epoch\" (will be stepped at the end of each epoch) \"scheduler_by_plateau\" (will step if accuracy plateaus. This requires you to write your own end-of-epoch hook, compute validation accuracy, and call trainer.step_lr_plateau_schedulers(validation_accuracy) . Or you can use HookContainer .) Here are some example valid lr_scheduler keys: trunk_scheduler_by_iteration metric_loss_scheduler_by_epoch embedder_scheduler_by_plateau gradient_clippers : A dictionary of gradient clipping functions. Each function will be called before the optimizers. freeze_these : Optional. A list or tuple of the names of models or loss functions that should have their parameters frozen during training. These models will have requires_grad set to False, and their optimizers will not be stepped. freeze_trunk_batchnorm : If True, then the BatchNorm parameters of the trunk model will be frozen during training. label_hierarchy_level : If each sample in your dataset has multiple labels, then this integer argument can be used to select which \"level\" to use. This assumes that your labels are \"2-dimensional\" with shape (num_samples, num_hierarchy_levels). Leave this at the default value, 0, if your data does not have multiple labels per sample. dataloader_num_workers : The number of processes your dataloader will use to load data. data_and_label_getter : A function that takes the output of your dataset's __getitem__ function, and returns a tuple of (data, labels). If None, then it is assumed that __getitem__ returns (data, labels). dataset_labels : The labels for your dataset. Can be 1-dimensional (1 label per datapoint) or 2-dimensional, where each row represents a datapoint, and the columns are the multiple labels that the datapoint has. Labels can be integers or strings. This option needs to be specified only if set_min_label_to_zero is True. set_min_label_to_zero : If True, labels will be mapped such that they represent their rank in the label set. For example, if your dataset has labels 5, 10, 12, 13, then at each iteration, these would become 0, 1, 2, 3. You should also set this to True if you want to use string labels. In that case, 'dog', 'cat', 'monkey' would get mapped to 1, 0, 2. If True, you must pass in dataset_labels (see above). The default is False. end_of_iteration_hook : This is an optional function that has one input argument (the trainer object), and performs some action (e.g. logging data) at the end of every iteration. Here are some things you might want to log: trainer.losses : this dictionary contains all loss values at the current iteration. trainer.loss_funcs and trainer.mining_funcs : these dictionaries contain the loss and mining functions. All loss and mining functions in pytorch-metric-learning have an attribute called record_these . This attribute is a list of strings, which are the names of other attributes that are worth recording for the purpose of analysis. For example, the record_these list for TripletMarginLoss is [\"avg_embedding_norm, \"num_non_zero_triplets\"] , so at each iteration you could log the value of trainer.loss_funcs[\"metric_loss\"].avg_embedding_norm and trainer.loss_funcs[\"metric_loss\"].num_non_zero_triplets . To accomplish this programmatically, you can loop through record_these and use the python function getattr to retrieve the attribute value. If you want ready-to-use hooks, take a look at the logging_presets module . end_of_epoch_hook : This is an optional function that operates like end_of_iteration_hook , except this occurs at the end of every epoch, so this might be a suitable place to run validation and save models. To end training early, your hook should return the boolean value False. Note, it must specifically return False , not None , 0 , [] etc. For this hook, you might want to access the following dictionaries: trainer.models , trainer.optimizers , trainer.lr_schedulers , trainer.loss_funcs , and trainer.mining_funcs . If you want ready-to-use hooks, take a look at the logging_presets module . MetricLossOnly \u00b6 This trainer just computes a metric loss from the output of your embedder network. See the example notebook . trainers . MetricLossOnly ( * args , ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model} Optionally include \"embedder\": embedder_model loss_funcs : Must have the following form: {\"metric_loss\": loss_func} TrainWithClassifier \u00b6 This trainer is for the case where your architecture is trunk -> embedder -> classifier. It applies a metric loss to the output of the embedder network, and a classification loss to the output of the classifier network. See the example notebook . trainers . TrainWithClassifier ( * args , ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"classifier\": classifier_model} Optionally include \"embedder\": embedder_model loss_funcs : Must have the following form: {\"metric_loss\": loss_func1, \"classifier_loss\": loss_func2} CascadedEmbeddings \u00b6 This trainer is a generalization of Hard-Aware Deeply Cascaded Embedding . It splits the output of your embedder network, computing a separate loss for each section. In other words, the output of your embedder should be the concatenation of your cascaded models. See the example notebook . trainers . CascadedEmbeddings ( embedding_sizes , * args , ** kwargs ) Parameters : embedding_sizes: A list of integers, which represent the size of the output of each cascaded model. Requirements : models : Must have the following form: {\"trunk\": trunk_model} Optionally include \"embedder\": embedder_model Optionally include key:values of the form \"classifier_%d\": classifier_model_%d. The integer appended to \"classifier_\" represents the cascaded model that the classifier will be appended to. For example, if the dictionary has classifier_0 and classifier_2, then the 0th and 2nd cascaded models will have classifier_model_0 and classifier_model_2 appended to them. loss_funcs : Must have the following form: {\"metric_loss_%d\": metric_loss_func_%d} Optionally include key:values of the form \"classifier_loss_%d\": classifier_loss_func_%d. The appended integer represents which cascaded model the loss applies to. mining_funcs : Must have the following form: {\"tuple_miner_%d\": mining_func_%d} Optionally include \"subset_batch_miner\": subset_batch_miner DeepAdversarialMetricLearning \u00b6 This is an implementation of Deep Adversarial Metric Learning . See the example notebook . trainers . DeepAdversarialMetricLearning ( metric_alone_epochs = 0 , g_alone_epochs = 0 , g_triplets_per_anchor = 100 , * args , ** kwargs ): Parameters : metric_alone_epochs : At the beginning of training, this many epochs will consist of only the metric_loss. g_alone_epochs : After metric_alone_epochs, this many epochs will consist of only the adversarial generator loss. g_triplets_per_anchor : The number of real triplets per sample that should be passed into the generator. For each real triplet, the generator will output a synthetic triplet. Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"generator\": generator_model} Optionally include \"embedder\": embedder_model Optionally include \"classifier\": classifier_model The input size to the generator must be 3*(size of trunk_model output). The output size must be (size of trunk_model output). loss_funcs : Must have the following form: {\"metric_loss\": metric_loss, \"g_adv_loss\": g_adv_loss, \"synth_loss\": synth_loss} Optionally include \"classifier_loss\": classifier_loss metric_loss applies to the embeddings of real data. g_adv_loss is the adversarial generator loss. Currently, only TripletMarginLoss and AngularLoss are supported synth_loss applies to the embeddings of the synthetic generator triplets. loss_weights : Must be one of the following: None {\"metric_loss\": weight1, \"g_adv_loss\": weight2, \"synth_loss\": weight3, \"g_reg_loss\": weight4, \"g_hard_lss\": weight5} Optionally include \"classifier_loss\": classifier_loss \"g_reg_loss\" and \"g_hard_loss\" refer to the regularization losses described in the paper. UnsupervisedEmbeddingsUsingAugmentations \u00b6 This is an implementation of a general approach that has been used in recent unsupervised learning papers, e.g. Unsupervised Embedding Learning via Invariant and Spreading Instance Feature and Unsupervised Deep Metric Learning via Auxiliary Rotation Loss . The idea is that augmented versions of a datapoint should be close to each other in the embedding space. trainers . UnsupervisedEmbeddingsUsingAugmentations ( transforms , data_and_label_setter = None , * args , ** kwargs ) Parameters : transforms : A list of transforms. For every sample in a batch, each transform will be applied to the sample. If there are N transforms and the batch size is B, then there will be a total of B*N augmented samples. data_and_label_setter : A function that takes in a tuple of (augmented_data, pseudo_labels) and outputs whatever is expected by self.data_and_label_getter. TwoStreamMetricLoss \u00b6 This trainer is the same as MetricLossOnly but operates on separate streams of anchors and positives/negatives. The supplied dataset must return (anchor, positive, label) . Given a batch of (anchor, positive, label) , triplets are formed using anchor as the anchor, and positive as either the positive or negative. See the example notebook . trainers . TwoStreamMetricLoss ( * args , ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model} Optionally include \"embedder\": embedder_model loss_funcs : Must have the following form: {\"metric_loss\": loss_func} mining_funcs : Only tuple miners are supported","title":"Trainers"},{"location":"trainers/#trainers","text":"Trainers exist in this library because some metric learning algorithms are more than just loss or mining functions. Some algorithms require additional networks, data augmentations, learning rate schedules etc. The goal of the trainers module is to provide access to these type of metric learning algorithms. In general, trainers are used as follows: from pytorch_metric_learning import trainers t = trainers . SomeTrainingFunction ( * args , ** kwargs ) t . train ( num_epochs = 10 )","title":"Trainers"},{"location":"trainers/#basetrainer","text":"All trainers extend this class and therefore inherit its __init__ arguments. trainers . BaseTrainer ( models , optimizers , batch_size , loss_funcs , mining_funcs , dataset , iterations_per_epoch = None , data_device = None , loss_weights = None , sampler = None , collate_fn = None , lr_schedulers = None , gradient_clippers = None , freeze_these = (), freeze_trunk_batchnorm = False , label_hierarchy_level = 0 , dataloader_num_workers = 32 , data_and_label_getter = None , dataset_labels = None , set_min_label_to_zero = False , end_of_iteration_hook = None , end_of_epoch_hook = None ) Parameters : models : A dictionary of the form: {\"trunk\": trunk_model, \"embedder\": embedder_model} The \"embedder\" key is optional. optimizers : A dictionary mapping strings to optimizers. The base class does not require any specific keys. For example, you could provide an empty dictionary, in which case no optimization will happen. Or you could provide just an optimizer for your trunk_model. But most likely, you'll want to pass in: {\"trunk_optimizer\": trunk_optimizer, \"embedder_optimizer\": embedder_optimizer}. batch_size : The number of elements that are retrieved at each iteration. loss_funcs : A dictionary mapping strings to loss functions. The required keys depend on the training method, but all methods are likely to require at least: {\"metric_loss\": loss_func}. mining_funcs : A dictionary mapping strings to mining functions. Pass in an empty dictionary, or one or more of the following keys: {\"subset_batch_miner\": mining_func1, \"tuple_miner\": mining_func2} dataset : The dataset you want to train on. Note that training methods do not perform validation, so do not pass in your validation or test set. data_device : The device that you want to put batches of data on. If not specified, the trainer will put the data on any available GPUs. iterations_per_epoch : Optional. If you don't specify iterations_per_epoch : 1 epoch = 1 pass through the dataloader iterator. If sampler=None , then 1 pass through the iterator is 1 pass through the dataset. If you use a sampler, then 1 pass through the iterator is 1 pass through the iterable returned by the sampler. For samplers like MPerClassSampler or some offline mining method, the iterable returned might be very long or very short etc, and might not be related to the length of the dataset. The length of the epoch might vary each time the sampler creates a new iterable. In these cases, it can be useful to specify iterations_per_epoch so that each \"epoch\" is just a fixed number of iterations. The definition of epoch matters because there's various things like LR schedulers and hooks that depend on an epoch ending. loss_weights : A dictionary mapping loss names to numbers. Each loss will be multiplied by the corresponding value in the dictionary. If not specified, then no loss weighting will occur. If not specified, then the original labels are used. sampler : The sampler used by the dataloader. If not specified, then random sampling will be used. collate_fn : The collate function used by the dataloader. lr_scheduers : A dictionary of PyTorch learning rate schedulers. Your keys should be strings of the form <model>_<step_type> , where <model> is a key that comes from either the models or loss_funcs dictionary, and <step_type> is one of the following: \"scheduler_by_iteration\" (will be stepped at every iteration) \"scheduler_by_epoch\" (will be stepped at the end of each epoch) \"scheduler_by_plateau\" (will step if accuracy plateaus. This requires you to write your own end-of-epoch hook, compute validation accuracy, and call trainer.step_lr_plateau_schedulers(validation_accuracy) . Or you can use HookContainer .) Here are some example valid lr_scheduler keys: trunk_scheduler_by_iteration metric_loss_scheduler_by_epoch embedder_scheduler_by_plateau gradient_clippers : A dictionary of gradient clipping functions. Each function will be called before the optimizers. freeze_these : Optional. A list or tuple of the names of models or loss functions that should have their parameters frozen during training. These models will have requires_grad set to False, and their optimizers will not be stepped. freeze_trunk_batchnorm : If True, then the BatchNorm parameters of the trunk model will be frozen during training. label_hierarchy_level : If each sample in your dataset has multiple labels, then this integer argument can be used to select which \"level\" to use. This assumes that your labels are \"2-dimensional\" with shape (num_samples, num_hierarchy_levels). Leave this at the default value, 0, if your data does not have multiple labels per sample. dataloader_num_workers : The number of processes your dataloader will use to load data. data_and_label_getter : A function that takes the output of your dataset's __getitem__ function, and returns a tuple of (data, labels). If None, then it is assumed that __getitem__ returns (data, labels). dataset_labels : The labels for your dataset. Can be 1-dimensional (1 label per datapoint) or 2-dimensional, where each row represents a datapoint, and the columns are the multiple labels that the datapoint has. Labels can be integers or strings. This option needs to be specified only if set_min_label_to_zero is True. set_min_label_to_zero : If True, labels will be mapped such that they represent their rank in the label set. For example, if your dataset has labels 5, 10, 12, 13, then at each iteration, these would become 0, 1, 2, 3. You should also set this to True if you want to use string labels. In that case, 'dog', 'cat', 'monkey' would get mapped to 1, 0, 2. If True, you must pass in dataset_labels (see above). The default is False. end_of_iteration_hook : This is an optional function that has one input argument (the trainer object), and performs some action (e.g. logging data) at the end of every iteration. Here are some things you might want to log: trainer.losses : this dictionary contains all loss values at the current iteration. trainer.loss_funcs and trainer.mining_funcs : these dictionaries contain the loss and mining functions. All loss and mining functions in pytorch-metric-learning have an attribute called record_these . This attribute is a list of strings, which are the names of other attributes that are worth recording for the purpose of analysis. For example, the record_these list for TripletMarginLoss is [\"avg_embedding_norm, \"num_non_zero_triplets\"] , so at each iteration you could log the value of trainer.loss_funcs[\"metric_loss\"].avg_embedding_norm and trainer.loss_funcs[\"metric_loss\"].num_non_zero_triplets . To accomplish this programmatically, you can loop through record_these and use the python function getattr to retrieve the attribute value. If you want ready-to-use hooks, take a look at the logging_presets module . end_of_epoch_hook : This is an optional function that operates like end_of_iteration_hook , except this occurs at the end of every epoch, so this might be a suitable place to run validation and save models. To end training early, your hook should return the boolean value False. Note, it must specifically return False , not None , 0 , [] etc. For this hook, you might want to access the following dictionaries: trainer.models , trainer.optimizers , trainer.lr_schedulers , trainer.loss_funcs , and trainer.mining_funcs . If you want ready-to-use hooks, take a look at the logging_presets module .","title":"BaseTrainer"},{"location":"trainers/#metriclossonly","text":"This trainer just computes a metric loss from the output of your embedder network. See the example notebook . trainers . MetricLossOnly ( * args , ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model} Optionally include \"embedder\": embedder_model loss_funcs : Must have the following form: {\"metric_loss\": loss_func}","title":"MetricLossOnly"},{"location":"trainers/#trainwithclassifier","text":"This trainer is for the case where your architecture is trunk -> embedder -> classifier. It applies a metric loss to the output of the embedder network, and a classification loss to the output of the classifier network. See the example notebook . trainers . TrainWithClassifier ( * args , ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"classifier\": classifier_model} Optionally include \"embedder\": embedder_model loss_funcs : Must have the following form: {\"metric_loss\": loss_func1, \"classifier_loss\": loss_func2}","title":"TrainWithClassifier"},{"location":"trainers/#cascadedembeddings","text":"This trainer is a generalization of Hard-Aware Deeply Cascaded Embedding . It splits the output of your embedder network, computing a separate loss for each section. In other words, the output of your embedder should be the concatenation of your cascaded models. See the example notebook . trainers . CascadedEmbeddings ( embedding_sizes , * args , ** kwargs ) Parameters : embedding_sizes: A list of integers, which represent the size of the output of each cascaded model. Requirements : models : Must have the following form: {\"trunk\": trunk_model} Optionally include \"embedder\": embedder_model Optionally include key:values of the form \"classifier_%d\": classifier_model_%d. The integer appended to \"classifier_\" represents the cascaded model that the classifier will be appended to. For example, if the dictionary has classifier_0 and classifier_2, then the 0th and 2nd cascaded models will have classifier_model_0 and classifier_model_2 appended to them. loss_funcs : Must have the following form: {\"metric_loss_%d\": metric_loss_func_%d} Optionally include key:values of the form \"classifier_loss_%d\": classifier_loss_func_%d. The appended integer represents which cascaded model the loss applies to. mining_funcs : Must have the following form: {\"tuple_miner_%d\": mining_func_%d} Optionally include \"subset_batch_miner\": subset_batch_miner","title":"CascadedEmbeddings"},{"location":"trainers/#deepadversarialmetriclearning","text":"This is an implementation of Deep Adversarial Metric Learning . See the example notebook . trainers . DeepAdversarialMetricLearning ( metric_alone_epochs = 0 , g_alone_epochs = 0 , g_triplets_per_anchor = 100 , * args , ** kwargs ): Parameters : metric_alone_epochs : At the beginning of training, this many epochs will consist of only the metric_loss. g_alone_epochs : After metric_alone_epochs, this many epochs will consist of only the adversarial generator loss. g_triplets_per_anchor : The number of real triplets per sample that should be passed into the generator. For each real triplet, the generator will output a synthetic triplet. Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"generator\": generator_model} Optionally include \"embedder\": embedder_model Optionally include \"classifier\": classifier_model The input size to the generator must be 3*(size of trunk_model output). The output size must be (size of trunk_model output). loss_funcs : Must have the following form: {\"metric_loss\": metric_loss, \"g_adv_loss\": g_adv_loss, \"synth_loss\": synth_loss} Optionally include \"classifier_loss\": classifier_loss metric_loss applies to the embeddings of real data. g_adv_loss is the adversarial generator loss. Currently, only TripletMarginLoss and AngularLoss are supported synth_loss applies to the embeddings of the synthetic generator triplets. loss_weights : Must be one of the following: None {\"metric_loss\": weight1, \"g_adv_loss\": weight2, \"synth_loss\": weight3, \"g_reg_loss\": weight4, \"g_hard_lss\": weight5} Optionally include \"classifier_loss\": classifier_loss \"g_reg_loss\" and \"g_hard_loss\" refer to the regularization losses described in the paper.","title":"DeepAdversarialMetricLearning"},{"location":"trainers/#unsupervisedembeddingsusingaugmentations","text":"This is an implementation of a general approach that has been used in recent unsupervised learning papers, e.g. Unsupervised Embedding Learning via Invariant and Spreading Instance Feature and Unsupervised Deep Metric Learning via Auxiliary Rotation Loss . The idea is that augmented versions of a datapoint should be close to each other in the embedding space. trainers . UnsupervisedEmbeddingsUsingAugmentations ( transforms , data_and_label_setter = None , * args , ** kwargs ) Parameters : transforms : A list of transforms. For every sample in a batch, each transform will be applied to the sample. If there are N transforms and the batch size is B, then there will be a total of B*N augmented samples. data_and_label_setter : A function that takes in a tuple of (augmented_data, pseudo_labels) and outputs whatever is expected by self.data_and_label_getter.","title":"UnsupervisedEmbeddingsUsingAugmentations"},{"location":"trainers/#twostreammetricloss","text":"This trainer is the same as MetricLossOnly but operates on separate streams of anchors and positives/negatives. The supplied dataset must return (anchor, positive, label) . Given a batch of (anchor, positive, label) , triplets are formed using anchor as the anchor, and positive as either the positive or negative. See the example notebook . trainers . TwoStreamMetricLoss ( * args , ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model} Optionally include \"embedder\": embedder_model loss_funcs : Must have the following form: {\"metric_loss\": loss_func} mining_funcs : Only tuple miners are supported","title":"TwoStreamMetricLoss"},{"location":"utils/","text":"Utils \u00b6 Utils contains various helpful functions and classes. Some of the more useful features are listed here. Accuracy Calculations \u00b6 The accuracy_calculator module contains functions for determining the quality of an embedding space. AccuracyCalculator \u00b6 This class computes several accuracy metrics given a query and reference embeddings. It can be easily extended to create custom accuracy metrics. from pytorch_metric_learning.utils import AccuracyCalculator AccuracyCalculator ( include = (), exclude = (), average_per_class = False ) Parameters : include : Optional. A list or tuple of strings, which are the names of metrics you want to calculate. If left empty, all default metrics will be calculated. exclude : Optional. A list or tuple of strings, which are the names of metrics you do not want to calculate. average_per_class : If True, the average accuracy per class is computed, and then the average of those averages is returned. This can be useful if your dataset has unbalanced classes. If False, the global average will be returned. Getting accuracy : Call the get_accuracy method to obtain a dictionary of accuracies. def get_accuracy ( self , query , reference , query_labels , reference_labels , embeddings_come_from_same_source , include = (), exclude = () ): # returns a dictionary mapping from metric names to accuracy values # The default metrics are: # \"NMI\" (Normalized Mutual Information) # \"AMI\" (Adjusted Mutual Information) # \"precision_at_1\" # \"r_precision\" # \"mean_average_precision_at_r\" query : A 2D numpy array of size (Nq, D) , where Nq is the number of query samples. For each query sample, nearest neighbors are retrieved and accuracy is computed. reference : A 2D numpy array of size (Nr, D) , where Nr is the number of reference samples. This is where nearest neighbors are retrieved from. query_labels : A 1D numpy array of size (Nq) . Each element should be an integer representing the sample's label. reference_labels : A 1D numpy array of size (Nr) . Each element should be an integer representing the sample's label. embeddings_come_from_same_source : Set to True if query is a subset of reference or if query is reference . Set to False otherwise. include : Optional. A list or tuple of strings, which are the names of metrics you want to calculate. If left empty, all metrics specified during initialization will be calculated. exclude : Optional. A list or tuple of strings, which are the names of metrics you do not want to calculate. Adding custom accuracy metrics Let's say you want to use the existing metrics but also compute precision @ 2, and a fancy mutual info method. You can extend the existing class, and write methods that start with the keyword calculate_ from pytorch_metric_learning.utils import accuracy_calculator , AccuracyCalculator class YourCalculator ( AccuracyCalculator ): def calculate_precision_at_2 ( self , knn_labels , query_labels , ** kwargs ): return accuracy_calculator . precision_at_k ( knn_labels , query_labels [:, None ], 2 ) def calculate_fancy_mutual_info ( self , query_labels , cluster_labels , ** kwargs ): return fancy_computations def requires_clustering ( self ): return super () . requires_clustering () + [ \"fancy_mutual_info\" ] def requires_knn ( self ): return super () . requires_knn () + [ \"precision_at_2\" ] Any method that starts with \"calculate_\" will be passed the following kwargs: kwargs = { \"query\" : query , # query embeddings \"reference\" : reference , # reference embeddings \"query_labels\" : query_labels , \"reference_labels\" : reference_labels , \"embeddings_come_from_same_source\" : e , # True if query is reference, or if query is a subset of reference. \"label_counts\" : label_counts , # a dictionary mapping from reference labels to the number of times they occur \"knn_labels\" : knn_labels } # A 2d array where each row is the labels of the nearest neighbors of each query. The neighbors are retrieved from the reference set If your method requires cluster labels, then append your method's name to the requires_clustering list, via super() . Then, if any of your methods need cluster labels, self.get_cluster_labels() will be called, and the kwargs will include cluster_labels . Likewise for computing k-nearest-neighbors. Now when get_accuracy is called, the returned dictionary will contain precision_at_2 and fancy_mutual_info : calculator = YourCalculator () acc_dict = calculator . get_accuracy ( query_embeddings , reference_embeddings , query_labels , reference_labels , embeddings_come_from_same_source = True ) # Now acc_dict contains the metrics \"precision_at_2\" and \"fancy_mutual_info\" # in addition to the original metrics from AccuracyCalculator You can use your custom calculator with the tester classes as well, by passing it in as an init argument. (By default, the testers use AccuracyCalculator.) from pytorch_metric_learning import testers t = testers . GlobalEmbeddingSpaceTester ( ... , accuracy_calculator = YourCalculator ()) Logging Presets \u00b6 The logging_presets module contains ready-to-use hooks for logging data, validating and saving your models, and early stoppage during training. It requires the record-keeper and tensorboard packages, which can be installed with pip: pip install record-keeper tensorboard Here's how you can use it in conjunction with a trainer and tester: import pytorch_metric_learning.utils.logging_presets as LP log_folder , tensorboard_folder = \"example_logs\" , \"example_tensorboard\" record_keeper , _ , _ = LP . get_record_keeper ( log_folder , tensorboard_folder ) hooks = LP . get_hook_container ( record_keeper ) dataset_dict = { \"val\" : val_dataset } model_folder = \"example_saved_models\" # Create the tester tester = testers . GlobalEmbeddingSpaceTester ( end_of_testing_hook = hooks . end_of_testing_hook ) end_of_epoch_hook = hooks . end_of_epoch_hook ( tester , dataset_dict , model_folder ) trainer = trainers . MetricLossOnly ( models , optimizers , batch_size , loss_funcs , mining_funcs , train_dataset , sampler = sampler , end_of_iteration_hook = hooks . end_of_iteration_hook , end_of_epoch_hook = end_of_epoch_hook ) trainer . train ( num_epochs = num_epochs ) With the provided hooks, data from both the training and validation stages will be saved in csv, sqlite, and tensorboard format, and models and optimizers will be saved in the specified model folder. See the example notebooks for complete examples. Read the next section to learn more about the provided hooks. HookContainer \u00b6 This class contains ready-to-use hooks to be used by trainers and testers. import pytorch_metric_learning.utils.logging_presets as LP LP . HookContainer ( record_keeper , record_group_name_prefix = None , primary_metric = \"mean_average_precision_at_r\" , validation_split_name = \"val\" , save_custom_figures = False ) Parameters : record_keeper : A record-keeper object. Install: pip install record-keeper tensorboard . record_group_name_prefix : A string which will be prepended to all record names and tensorboard tags. primary_metric : A string that specifies the accuracy metric which will be used to determine the best checkpoint. Must be one of: mean_average_precision_at_r r_precision precision_at_1 NMI validation_split_name : Optional. Default value is \"val\". The name of your validation set in dataset_dict . save_custom_figures : Optional. If True, records that consist of a tensor at each iteration (rather than just a scalar), will be plotted on tensorboard. Important functions : end_of_iteration_hook : This function records data about models, optimizers, and loss and mining functions. You can pass this function directly into a trainer object. end_of_epoch_hook : This function runs validation and saves models. This function returns the actual hook, i.e. you must pass in the following arguments to obtain the hook. tester : A tester object. dataset_dict : A dictionary mapping from split names to PyTorch datasets. For example: {\"train\": train_dataset, \"val\": val_dataset} model_folder : A string which is the folder path where models, optimizers etc. will be saved. test_interval : Optional. Default value is 1. Validation will be run every test_interval epochs. patience : Optional. Default value is None. If not None, training will end early if epoch - best_epoch > patience . test_collate_fn : Optional. Default value is None. This is the collate function used by the dataloader during testing. end_of_testing_hook : This function records accuracy metrics. You can pass this function directly into a tester object. Useful methods : Getting loss history: # Get a dictionary mapping from loss names to lists loss_histories = hooks . get_loss_history () # You can also specify which loss histories you want # It will still return a dictionary. In this case, the dictionary will contain only \"total_loss\" loss_histories = hooks . get_loss_history ( loss_names = [ \"total_loss\" ]) Getting accuracy history # The first argument is the tester object. The second is the split name. # Get a dictionary containing the keys \"epoch\" and the primary metric # The values are lists acc_histories = hooks . get_accuracy_history ( tester , \"val\" ) # Get all accuracy histories acc_histories = hooks . get_accuracy_history ( tester , \"val\" , return_all_metrics = True ) # Get a specific set of accuracy histories acc_histories = hooks . get_accuracy_history ( tester , \"val\" , metrics = [ \"AMI\" , \"NMI\" ])","title":"Utils"},{"location":"utils/#utils","text":"Utils contains various helpful functions and classes. Some of the more useful features are listed here.","title":"Utils"},{"location":"utils/#accuracy-calculations","text":"The accuracy_calculator module contains functions for determining the quality of an embedding space.","title":"Accuracy Calculations"},{"location":"utils/#accuracycalculator","text":"This class computes several accuracy metrics given a query and reference embeddings. It can be easily extended to create custom accuracy metrics. from pytorch_metric_learning.utils import AccuracyCalculator AccuracyCalculator ( include = (), exclude = (), average_per_class = False ) Parameters : include : Optional. A list or tuple of strings, which are the names of metrics you want to calculate. If left empty, all default metrics will be calculated. exclude : Optional. A list or tuple of strings, which are the names of metrics you do not want to calculate. average_per_class : If True, the average accuracy per class is computed, and then the average of those averages is returned. This can be useful if your dataset has unbalanced classes. If False, the global average will be returned. Getting accuracy : Call the get_accuracy method to obtain a dictionary of accuracies. def get_accuracy ( self , query , reference , query_labels , reference_labels , embeddings_come_from_same_source , include = (), exclude = () ): # returns a dictionary mapping from metric names to accuracy values # The default metrics are: # \"NMI\" (Normalized Mutual Information) # \"AMI\" (Adjusted Mutual Information) # \"precision_at_1\" # \"r_precision\" # \"mean_average_precision_at_r\" query : A 2D numpy array of size (Nq, D) , where Nq is the number of query samples. For each query sample, nearest neighbors are retrieved and accuracy is computed. reference : A 2D numpy array of size (Nr, D) , where Nr is the number of reference samples. This is where nearest neighbors are retrieved from. query_labels : A 1D numpy array of size (Nq) . Each element should be an integer representing the sample's label. reference_labels : A 1D numpy array of size (Nr) . Each element should be an integer representing the sample's label. embeddings_come_from_same_source : Set to True if query is a subset of reference or if query is reference . Set to False otherwise. include : Optional. A list or tuple of strings, which are the names of metrics you want to calculate. If left empty, all metrics specified during initialization will be calculated. exclude : Optional. A list or tuple of strings, which are the names of metrics you do not want to calculate. Adding custom accuracy metrics Let's say you want to use the existing metrics but also compute precision @ 2, and a fancy mutual info method. You can extend the existing class, and write methods that start with the keyword calculate_ from pytorch_metric_learning.utils import accuracy_calculator , AccuracyCalculator class YourCalculator ( AccuracyCalculator ): def calculate_precision_at_2 ( self , knn_labels , query_labels , ** kwargs ): return accuracy_calculator . precision_at_k ( knn_labels , query_labels [:, None ], 2 ) def calculate_fancy_mutual_info ( self , query_labels , cluster_labels , ** kwargs ): return fancy_computations def requires_clustering ( self ): return super () . requires_clustering () + [ \"fancy_mutual_info\" ] def requires_knn ( self ): return super () . requires_knn () + [ \"precision_at_2\" ] Any method that starts with \"calculate_\" will be passed the following kwargs: kwargs = { \"query\" : query , # query embeddings \"reference\" : reference , # reference embeddings \"query_labels\" : query_labels , \"reference_labels\" : reference_labels , \"embeddings_come_from_same_source\" : e , # True if query is reference, or if query is a subset of reference. \"label_counts\" : label_counts , # a dictionary mapping from reference labels to the number of times they occur \"knn_labels\" : knn_labels } # A 2d array where each row is the labels of the nearest neighbors of each query. The neighbors are retrieved from the reference set If your method requires cluster labels, then append your method's name to the requires_clustering list, via super() . Then, if any of your methods need cluster labels, self.get_cluster_labels() will be called, and the kwargs will include cluster_labels . Likewise for computing k-nearest-neighbors. Now when get_accuracy is called, the returned dictionary will contain precision_at_2 and fancy_mutual_info : calculator = YourCalculator () acc_dict = calculator . get_accuracy ( query_embeddings , reference_embeddings , query_labels , reference_labels , embeddings_come_from_same_source = True ) # Now acc_dict contains the metrics \"precision_at_2\" and \"fancy_mutual_info\" # in addition to the original metrics from AccuracyCalculator You can use your custom calculator with the tester classes as well, by passing it in as an init argument. (By default, the testers use AccuracyCalculator.) from pytorch_metric_learning import testers t = testers . GlobalEmbeddingSpaceTester ( ... , accuracy_calculator = YourCalculator ())","title":"AccuracyCalculator"},{"location":"utils/#logging-presets","text":"The logging_presets module contains ready-to-use hooks for logging data, validating and saving your models, and early stoppage during training. It requires the record-keeper and tensorboard packages, which can be installed with pip: pip install record-keeper tensorboard Here's how you can use it in conjunction with a trainer and tester: import pytorch_metric_learning.utils.logging_presets as LP log_folder , tensorboard_folder = \"example_logs\" , \"example_tensorboard\" record_keeper , _ , _ = LP . get_record_keeper ( log_folder , tensorboard_folder ) hooks = LP . get_hook_container ( record_keeper ) dataset_dict = { \"val\" : val_dataset } model_folder = \"example_saved_models\" # Create the tester tester = testers . GlobalEmbeddingSpaceTester ( end_of_testing_hook = hooks . end_of_testing_hook ) end_of_epoch_hook = hooks . end_of_epoch_hook ( tester , dataset_dict , model_folder ) trainer = trainers . MetricLossOnly ( models , optimizers , batch_size , loss_funcs , mining_funcs , train_dataset , sampler = sampler , end_of_iteration_hook = hooks . end_of_iteration_hook , end_of_epoch_hook = end_of_epoch_hook ) trainer . train ( num_epochs = num_epochs ) With the provided hooks, data from both the training and validation stages will be saved in csv, sqlite, and tensorboard format, and models and optimizers will be saved in the specified model folder. See the example notebooks for complete examples. Read the next section to learn more about the provided hooks.","title":"Logging Presets"},{"location":"utils/#hookcontainer","text":"This class contains ready-to-use hooks to be used by trainers and testers. import pytorch_metric_learning.utils.logging_presets as LP LP . HookContainer ( record_keeper , record_group_name_prefix = None , primary_metric = \"mean_average_precision_at_r\" , validation_split_name = \"val\" , save_custom_figures = False ) Parameters : record_keeper : A record-keeper object. Install: pip install record-keeper tensorboard . record_group_name_prefix : A string which will be prepended to all record names and tensorboard tags. primary_metric : A string that specifies the accuracy metric which will be used to determine the best checkpoint. Must be one of: mean_average_precision_at_r r_precision precision_at_1 NMI validation_split_name : Optional. Default value is \"val\". The name of your validation set in dataset_dict . save_custom_figures : Optional. If True, records that consist of a tensor at each iteration (rather than just a scalar), will be plotted on tensorboard. Important functions : end_of_iteration_hook : This function records data about models, optimizers, and loss and mining functions. You can pass this function directly into a trainer object. end_of_epoch_hook : This function runs validation and saves models. This function returns the actual hook, i.e. you must pass in the following arguments to obtain the hook. tester : A tester object. dataset_dict : A dictionary mapping from split names to PyTorch datasets. For example: {\"train\": train_dataset, \"val\": val_dataset} model_folder : A string which is the folder path where models, optimizers etc. will be saved. test_interval : Optional. Default value is 1. Validation will be run every test_interval epochs. patience : Optional. Default value is None. If not None, training will end early if epoch - best_epoch > patience . test_collate_fn : Optional. Default value is None. This is the collate function used by the dataloader during testing. end_of_testing_hook : This function records accuracy metrics. You can pass this function directly into a tester object. Useful methods : Getting loss history: # Get a dictionary mapping from loss names to lists loss_histories = hooks . get_loss_history () # You can also specify which loss histories you want # It will still return a dictionary. In this case, the dictionary will contain only \"total_loss\" loss_histories = hooks . get_loss_history ( loss_names = [ \"total_loss\" ]) Getting accuracy history # The first argument is the tester object. The second is the split name. # Get a dictionary containing the keys \"epoch\" and the primary metric # The values are lists acc_histories = hooks . get_accuracy_history ( tester , \"val\" ) # Get all accuracy histories acc_histories = hooks . get_accuracy_history ( tester , \"val\" , return_all_metrics = True ) # Get a specific set of accuracy histories acc_histories = hooks . get_accuracy_history ( tester , \"val\" , metrics = [ \"AMI\" , \"NMI\" ])","title":"HookContainer"}]}